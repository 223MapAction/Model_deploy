{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hey! it's Map Action Map Action envisions a world where technology revolutionizes environmental management and rural & urban problem-solving, making sustainable living accessible to all communities in Mali and beyond. In a world increasingly challenged by environmental issues and urban complexities, Map Action envisions a future where cutting-edge technology and geospatial solutions empower communities, governments, and organizations. Our vision is to create a global society where sustainable urban and rural development and environmental stewardship are not only achievable but are the cornerstones of our collective well-being. Mission Statement Our mission is to develop, deploy, and promote open-source mapping tools and methodologies that enable individuals, communities, governments, and organizations to collaboratively identify, analyze, and solve critical environmental and urban challenges. Community Statement Map Action thrives on the strength of a diverse, inclusive community united by the goal of using technology for sustainable urban and environmental management. We commit to fostering an open, respectful environment where every voice is valued, and collaboration drives innovation. Together, we empower individuals and organizations to actively participate in crafting solutions that make a meaningful impact.","title":"Welcom"},{"location":"#hey-its-map-action","text":"Map Action envisions a world where technology revolutionizes environmental management and rural & urban problem-solving, making sustainable living accessible to all communities in Mali and beyond. In a world increasingly challenged by environmental issues and urban complexities, Map Action envisions a future where cutting-edge technology and geospatial solutions empower communities, governments, and organizations. Our vision is to create a global society where sustainable urban and rural development and environmental stewardship are not only achievable but are the cornerstones of our collective well-being.","title":"Hey! it's Map Action"},{"location":"api_route/","text":"chat_endpoint ( websocket ) async WebSocket endpoint for handling chat interactions and chat history deletion. Parameters: Name Type Description Default websocket WebSocket The WebSocket connection. required Raises: Type Description WebSocketDisconnect If the connection is closed. Source code in app/apis/main_router.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 @router . websocket ( \"/ws/chat\" ) async def chat_endpoint ( websocket : WebSocket ): \"\"\" WebSocket endpoint for handling chat interactions and chat history deletion. Args: websocket (WebSocket): The WebSocket connection. Raises: WebSocketDisconnect: If the connection is closed. \"\"\" logger . info ( f \"WebSocket connection attempt from { websocket . client . host } \" ) origin = websocket . headers . get ( \"origin\" ) logger . info ( f \"Received Origin: { origin } \" ) allowed_origins = [ \"http://197.155.176.134\" , \"http://localhost:3000\" , \"http://127.0.0.1\" , \"http://57.153.185.160\" , \"https://app.map-action.com\" , \"http://app.map-action.com\" , None , ] if origin not in allowed_origins : logger . warning ( f \"Connection rejected from origin: { origin } \" ) await websocket . close ( code = status . WS_1008_POLICY_VIOLATION ) return logger . info ( f \"Connection accepted from origin: { origin } \" ) await manager . connect ( websocket ) try : while True : data = await websocket . receive_json () action = data . get ( \"action\" ) incident_id = data . get ( \"incident_id\" ) session_id = data . get ( \"session_id\" ) if not incident_id or not session_id : logger . error ( \"Missing incident_id or session_id in message\" ) await websocket . send_json ({ \"error\" : \"Missing incident_id or session_id\" }) continue if action == \"delete_chat\" : # Handle chat history deletion chat_key = f \" { session_id }{ incident_id } \" query = \"\"\" DELETE FROM \"Mapapi_chathistory\" WHERE session_id = :session_id; \"\"\" values = { \"session_id\" : chat_key } try : await database . execute ( query = query , values = values ) logger . info ( f \"Chat history deleted for session { chat_key } \" ) # Clear in-memory chat history if chat_key in chat_histories : del chat_histories [ chat_key ] # Send a confirmation to the client await websocket . send_json ({ \"message\" : \"Chat history deleted successfully.\" }) except Exception as e : logger . error ( f \"Error deleting chat history: { e } \" ) await websocket . send_json ({ \"error\" : \"Error deleting chat history.\" }) else : # Fetch context from the database based on incident_id (similar to before) query = \"\"\" SELECT incident_type, context, impact_potentiel, piste_solution FROM \"Mapapi_prediction\" WHERE incident_id = :incident_id; \"\"\" values = { \"incident_id\" : incident_id } result = await database . fetch_one ( query = query , values = values ) if result : context_obj = { \"type_incident\" : result [ \"incident_type\" ], \"context\" : result [ \"context\" ], \"impact_potentiel\" : result [ \"impact_potentiel\" ], \"piste_solution\" : result [ \"piste_solution\" ], } context = json . dumps ( context_obj ) else : logger . error ( f \"No context found for incident_id: { incident_id } \" ) await websocket . send_json ({ \"error\" : \"No context found for the given incident_id\" }) await websocket . close ( code = status . WS_1008_POLICY_VIOLATION ) return chat_key = f \" { session_id }{ incident_id } \" # Initialize chat history if not present if chat_key not in chat_histories : history_query = \"\"\" SELECT question, answer FROM \"Mapapi_chathistory\" WHERE session_id = :session_id ORDER BY id ASC; \"\"\" history_values = { \"session_id\" : chat_key } history_results = await database . fetch_all ( query = history_query , values = history_values ) chat_histories [ chat_key ] = [ { \"role\" : \"user\" , \"content\" : record [ \"question\" ]} for record in history_results ] + [ { \"role\" : \"assistant\" , \"content\" : record [ \"answer\" ]} for record in history_results ] # Add the user's question to the chat history question = data . get ( \"question\" ) chat_histories [ chat_key ] . append ({ \"role\" : \"user\" , \"content\" : question }) # Get response from chat bot chatbot_response = chat_response ( question , context , chat_histories [ chat_key ]) # Append assistant's response to history chat_histories [ chat_key ] . append ({ \"role\" : \"assistant\" , \"content\" : chatbot_response }) # Send the response back through the WebSocket response_message = { \"incident_id\" : incident_id , \"session_id\" : session_id , \"question\" : question , \"answer\" : chatbot_response , } await websocket . send_json ( response_message ) # Save the chat history to the database await save_chat_history ( chat_key , question , chatbot_response ) except WebSocketDisconnect : manager . disconnect ( websocket ) logger . info ( f \"WebSocket disconnected from { websocket . client . host } \" ) except Exception as e : logger . error ( f \"WebSocket error: { e } \" ) await websocket . close ( code = status . WS_1011_INTERNAL_ERROR ) construct_image_url ( image_name ) Constructs the full URL for the image based on the image name. Parameters: Name Type Description Default image_name str The name or path of the image. required Returns: Name Type Description str str The full URL to access the image. Source code in app/apis/main_router.py 33 34 35 36 37 38 39 40 41 42 43 def construct_image_url ( image_name : str ) -> str : \"\"\" Constructs the full URL for the image based on the image name. Args: image_name (str): The name or path of the image. Returns: str: The full URL to access the image. \"\"\" return f \" { BASE_URL } / { image_name . split ( '/' )[ - 1 ] } \" fetch_image ( image_url ) async Fetches the image from the specified URL. Parameters: Name Type Description Default image_url str The URL of the image to fetch. required Returns: Name Type Description bytes bytes The binary content of the fetched image. Raises: Type Description HTTPException If the image cannot be fetched. Source code in app/apis/main_router.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 async def fetch_image ( image_url : str ) -> bytes : \"\"\" Fetches the image from the specified URL. Args: image_url (str): The URL of the image to fetch. Returns: bytes: The binary content of the fetched image. Raises: HTTPException: If the image cannot be fetched. \"\"\" try : response = requests . get ( image_url ) response . raise_for_status () return response . content except requests . RequestException as e : logger . error ( f \"Failed to fetch image from { image_url } : { str ( e ) } \" ) raise HTTPException ( status_code = 500 , detail = f \"Failed to fetch image: { str ( e ) } \" ) get_chat_history ( chat_key ) async Retrieves the chat history for a given chat_key. Parameters: Name Type Description Default chat_key str The unique identifier for the chat session. required Returns: Name Type Description list A list of chat messages in chronological order. Raises: Type Description HTTPException If there is an error fetching the chat history. Source code in app/apis/main_router.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 @router . get ( \"/MapApi/history/ {chat_key} \" ) async def get_chat_history ( chat_key : str ): \"\"\" Retrieves the chat history for a given chat_key. Args: chat_key (str): The unique identifier for the chat session. Returns: list: A list of chat messages in chronological order. Raises: HTTPException: If there is an error fetching the chat history. \"\"\" query = \"\"\" SELECT question, answer FROM \"Mapapi_chathistory\" WHERE session_id = :session_id ORDER BY id ASC; \"\"\" values = { \"session_id\" : chat_key } try : results = await database . fetch_all ( query = query , values = values ) # Format the results to interleave user and assistant messages formatted_history = [] for record in results : formatted_history . append ({ \"role\" : \"user\" , \"content\" : record [ \"question\" ]}) formatted_history . append ({ \"role\" : \"assistant\" , \"content\" : record [ \"answer\" ]}) return formatted_history except Exception as e : logger . error ( f \"Error fetching chat history: { e } \" ) raise HTTPException ( status_code = 500 , detail = \"Error fetching chat history\" ) index () Root endpoint to verify that the API is running. Returns: Name Type Description dict A message indicating the API is operational. Source code in app/apis/main_router.py 85 86 87 88 89 90 91 92 93 @router . get ( \"/\" ) def index (): \"\"\" Root endpoint to verify that the API is running. Returns: dict: A message indicating the API is operational. \"\"\" return { \"message\" : \"Map Action classification model\" } predict_incident_type ( data ) async Predicts the type of incident based on the provided image and other data. Parameters: Name Type Description Default data ImageModel The input data containing image name, sensitive structures, and incident ID. required Returns: Name Type Description JSONResponse The prediction results including incident type, probabilities, context, impact, and solution. Raises: Type Description HTTPException If any step in the prediction process fails. Source code in app/apis/main_router.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 @router . post ( \"/image/predict\" ) async def predict_incident_type ( data : ImageModel ): \"\"\" Predicts the type of incident based on the provided image and other data. Args: data (ImageModel): The input data containing image name, sensitive structures, and incident ID. Returns: JSONResponse: The prediction results including incident type, probabilities, context, impact, and solution. Raises: HTTPException: If any step in the prediction process fails. \"\"\" try : logger . info ( f \"Received request for image: { data . image_name } with sensitive structures: { data . sensitive_structures } and incident_id: { data . incident_id } \" ) image_url = construct_image_url ( data . image_name ) image = await fetch_image ( image_url ) # Perform prediction asynchronously using Celery prediction_task = perform_prediction . delay ( image ) try : prediction , probabilities = prediction_task . get ( timeout = 120 ) logger . info ( f \"Prediction successful: { prediction } with probabilities: { probabilities } \" ) if isinstance ( probabilities , np . ndarray ): probabilities = probabilities . tolist () except Exception as e : logger . error ( f \"Error during prediction task: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Error during prediction: { str ( e ) } \" ) # Fetch contextual information asynchronously using Celery context_task = fetch_contextual_information . delay ( prediction , data . sensitive_structures ) try : get_context , impact , piste_solution = context_task . get ( timeout = 120 ) logger . info ( f \"Context fetching successful: { get_context } , { impact } , { piste_solution } \" ) except Exception as e : logger . error ( f \"Error during context fetching task: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Error during context fetching: { str ( e ) } \" ) # Validate all required fields are present if not all ([ data . incident_id , prediction , piste_solution , impact , get_context ]): raise HTTPException ( status_code = 400 , detail = \"Missing required fields for database insertion.\" ) # Insert the prediction and context into the database query = \"\"\" INSERT INTO \"Mapapi_prediction\" (incident_id, incident_type, piste_solution, impact_potentiel, context) VALUES (:incident_id, :incident_type, :piste_solution, :impact_potentiel, :context); \"\"\" values = { \"incident_id\" : data . incident_id , \"incident_type\" : prediction , \"piste_solution\" : piste_solution , \"impact_potentiel\" : impact , \"context\" : get_context , } try : await database . execute ( query = query , values = values ) logger . info ( \"Database insertion successful\" ) except Exception as e : logger . error ( f \"Database error: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Database error: { str ( e ) } \" ) return JSONResponse ( content = { \"prediction\" : prediction , \"probabilities\" : probabilities , \"context\" : get_context , \"in_depth\" : impact , \"piste_solution\" : piste_solution , } ) except HTTPException as http_exc : # Re-raise HTTPExceptions to be handled by the global exception handler raise http_exc except Exception as e : logger . error ( f \"Unhandled exception: { e } \" ) raise HTTPException ( status_code = 500 , detail = str ( e )) sanitize_error_message ( message , sensitive_structures ) Sanitizes the error message by masking sensitive structures. Parameters: Name Type Description Default message str The original error message. required sensitive_structures List [ str ] A list of sensitive terms to mask. required Returns: Name Type Description str str The sanitized error message. Source code in app/apis/main_router.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def sanitize_error_message ( message : str , sensitive_structures : List [ str ]) -> str : \"\"\" Sanitizes the error message by masking sensitive structures. Args: message (str): The original error message. sensitive_structures (List[str]): A list of sensitive terms to mask. Returns: str: The sanitized error message. \"\"\" sanitized_message = message for structure in sensitive_structures : sanitized_message = sanitized_message . replace ( structure , \"***\" ) return sanitized_message save_chat_history ( chat_key , question , answer ) async Saves a single chat interaction to the database. Parameters: Name Type Description Default chat_key str The unique identifier for the chat session. required question str The user's question. required answer str The assistant's answer. required Raises: Type Description HTTPException If there is an error saving to the database. Source code in app/apis/main_router.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 async def save_chat_history ( chat_key : str , question : str , answer : str ): \"\"\" Saves a single chat interaction to the database. Args: chat_key (str): The unique identifier for the chat session. question (str): The user's question. answer (str): The assistant's answer. Raises: HTTPException: If there is an error saving to the database. \"\"\" query = \"\"\" INSERT INTO \"Mapapi_chathistory\" (session_id, question, answer) VALUES (:session_id, :question, :answer); \"\"\" values = { \"session_id\" : chat_key , \"question\" : question , \"answer\" : answer , } try : await database . execute ( query = query , values = values ) logger . info ( f \"Chat history saved for session { chat_key } \" ) except Exception as e : logger . error ( f \"Error saving chat history: { e } \" )","title":"Api route"},{"location":"api_route/#apis.main_router.chat_endpoint","text":"WebSocket endpoint for handling chat interactions and chat history deletion. Parameters: Name Type Description Default websocket WebSocket The WebSocket connection. required Raises: Type Description WebSocketDisconnect If the connection is closed. Source code in app/apis/main_router.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 @router . websocket ( \"/ws/chat\" ) async def chat_endpoint ( websocket : WebSocket ): \"\"\" WebSocket endpoint for handling chat interactions and chat history deletion. Args: websocket (WebSocket): The WebSocket connection. Raises: WebSocketDisconnect: If the connection is closed. \"\"\" logger . info ( f \"WebSocket connection attempt from { websocket . client . host } \" ) origin = websocket . headers . get ( \"origin\" ) logger . info ( f \"Received Origin: { origin } \" ) allowed_origins = [ \"http://197.155.176.134\" , \"http://localhost:3000\" , \"http://127.0.0.1\" , \"http://57.153.185.160\" , \"https://app.map-action.com\" , \"http://app.map-action.com\" , None , ] if origin not in allowed_origins : logger . warning ( f \"Connection rejected from origin: { origin } \" ) await websocket . close ( code = status . WS_1008_POLICY_VIOLATION ) return logger . info ( f \"Connection accepted from origin: { origin } \" ) await manager . connect ( websocket ) try : while True : data = await websocket . receive_json () action = data . get ( \"action\" ) incident_id = data . get ( \"incident_id\" ) session_id = data . get ( \"session_id\" ) if not incident_id or not session_id : logger . error ( \"Missing incident_id or session_id in message\" ) await websocket . send_json ({ \"error\" : \"Missing incident_id or session_id\" }) continue if action == \"delete_chat\" : # Handle chat history deletion chat_key = f \" { session_id }{ incident_id } \" query = \"\"\" DELETE FROM \"Mapapi_chathistory\" WHERE session_id = :session_id; \"\"\" values = { \"session_id\" : chat_key } try : await database . execute ( query = query , values = values ) logger . info ( f \"Chat history deleted for session { chat_key } \" ) # Clear in-memory chat history if chat_key in chat_histories : del chat_histories [ chat_key ] # Send a confirmation to the client await websocket . send_json ({ \"message\" : \"Chat history deleted successfully.\" }) except Exception as e : logger . error ( f \"Error deleting chat history: { e } \" ) await websocket . send_json ({ \"error\" : \"Error deleting chat history.\" }) else : # Fetch context from the database based on incident_id (similar to before) query = \"\"\" SELECT incident_type, context, impact_potentiel, piste_solution FROM \"Mapapi_prediction\" WHERE incident_id = :incident_id; \"\"\" values = { \"incident_id\" : incident_id } result = await database . fetch_one ( query = query , values = values ) if result : context_obj = { \"type_incident\" : result [ \"incident_type\" ], \"context\" : result [ \"context\" ], \"impact_potentiel\" : result [ \"impact_potentiel\" ], \"piste_solution\" : result [ \"piste_solution\" ], } context = json . dumps ( context_obj ) else : logger . error ( f \"No context found for incident_id: { incident_id } \" ) await websocket . send_json ({ \"error\" : \"No context found for the given incident_id\" }) await websocket . close ( code = status . WS_1008_POLICY_VIOLATION ) return chat_key = f \" { session_id }{ incident_id } \" # Initialize chat history if not present if chat_key not in chat_histories : history_query = \"\"\" SELECT question, answer FROM \"Mapapi_chathistory\" WHERE session_id = :session_id ORDER BY id ASC; \"\"\" history_values = { \"session_id\" : chat_key } history_results = await database . fetch_all ( query = history_query , values = history_values ) chat_histories [ chat_key ] = [ { \"role\" : \"user\" , \"content\" : record [ \"question\" ]} for record in history_results ] + [ { \"role\" : \"assistant\" , \"content\" : record [ \"answer\" ]} for record in history_results ] # Add the user's question to the chat history question = data . get ( \"question\" ) chat_histories [ chat_key ] . append ({ \"role\" : \"user\" , \"content\" : question }) # Get response from chat bot chatbot_response = chat_response ( question , context , chat_histories [ chat_key ]) # Append assistant's response to history chat_histories [ chat_key ] . append ({ \"role\" : \"assistant\" , \"content\" : chatbot_response }) # Send the response back through the WebSocket response_message = { \"incident_id\" : incident_id , \"session_id\" : session_id , \"question\" : question , \"answer\" : chatbot_response , } await websocket . send_json ( response_message ) # Save the chat history to the database await save_chat_history ( chat_key , question , chatbot_response ) except WebSocketDisconnect : manager . disconnect ( websocket ) logger . info ( f \"WebSocket disconnected from { websocket . client . host } \" ) except Exception as e : logger . error ( f \"WebSocket error: { e } \" ) await websocket . close ( code = status . WS_1011_INTERNAL_ERROR )","title":"chat_endpoint"},{"location":"api_route/#apis.main_router.construct_image_url","text":"Constructs the full URL for the image based on the image name. Parameters: Name Type Description Default image_name str The name or path of the image. required Returns: Name Type Description str str The full URL to access the image. Source code in app/apis/main_router.py 33 34 35 36 37 38 39 40 41 42 43 def construct_image_url ( image_name : str ) -> str : \"\"\" Constructs the full URL for the image based on the image name. Args: image_name (str): The name or path of the image. Returns: str: The full URL to access the image. \"\"\" return f \" { BASE_URL } / { image_name . split ( '/' )[ - 1 ] } \"","title":"construct_image_url"},{"location":"api_route/#apis.main_router.fetch_image","text":"Fetches the image from the specified URL. Parameters: Name Type Description Default image_url str The URL of the image to fetch. required Returns: Name Type Description bytes bytes The binary content of the fetched image. Raises: Type Description HTTPException If the image cannot be fetched. Source code in app/apis/main_router.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 async def fetch_image ( image_url : str ) -> bytes : \"\"\" Fetches the image from the specified URL. Args: image_url (str): The URL of the image to fetch. Returns: bytes: The binary content of the fetched image. Raises: HTTPException: If the image cannot be fetched. \"\"\" try : response = requests . get ( image_url ) response . raise_for_status () return response . content except requests . RequestException as e : logger . error ( f \"Failed to fetch image from { image_url } : { str ( e ) } \" ) raise HTTPException ( status_code = 500 , detail = f \"Failed to fetch image: { str ( e ) } \" )","title":"fetch_image"},{"location":"api_route/#apis.main_router.get_chat_history","text":"Retrieves the chat history for a given chat_key. Parameters: Name Type Description Default chat_key str The unique identifier for the chat session. required Returns: Name Type Description list A list of chat messages in chronological order. Raises: Type Description HTTPException If there is an error fetching the chat history. Source code in app/apis/main_router.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 @router . get ( \"/MapApi/history/ {chat_key} \" ) async def get_chat_history ( chat_key : str ): \"\"\" Retrieves the chat history for a given chat_key. Args: chat_key (str): The unique identifier for the chat session. Returns: list: A list of chat messages in chronological order. Raises: HTTPException: If there is an error fetching the chat history. \"\"\" query = \"\"\" SELECT question, answer FROM \"Mapapi_chathistory\" WHERE session_id = :session_id ORDER BY id ASC; \"\"\" values = { \"session_id\" : chat_key } try : results = await database . fetch_all ( query = query , values = values ) # Format the results to interleave user and assistant messages formatted_history = [] for record in results : formatted_history . append ({ \"role\" : \"user\" , \"content\" : record [ \"question\" ]}) formatted_history . append ({ \"role\" : \"assistant\" , \"content\" : record [ \"answer\" ]}) return formatted_history except Exception as e : logger . error ( f \"Error fetching chat history: { e } \" ) raise HTTPException ( status_code = 500 , detail = \"Error fetching chat history\" )","title":"get_chat_history"},{"location":"api_route/#apis.main_router.index","text":"Root endpoint to verify that the API is running. Returns: Name Type Description dict A message indicating the API is operational. Source code in app/apis/main_router.py 85 86 87 88 89 90 91 92 93 @router . get ( \"/\" ) def index (): \"\"\" Root endpoint to verify that the API is running. Returns: dict: A message indicating the API is operational. \"\"\" return { \"message\" : \"Map Action classification model\" }","title":"index"},{"location":"api_route/#apis.main_router.predict_incident_type","text":"Predicts the type of incident based on the provided image and other data. Parameters: Name Type Description Default data ImageModel The input data containing image name, sensitive structures, and incident ID. required Returns: Name Type Description JSONResponse The prediction results including incident type, probabilities, context, impact, and solution. Raises: Type Description HTTPException If any step in the prediction process fails. Source code in app/apis/main_router.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 @router . post ( \"/image/predict\" ) async def predict_incident_type ( data : ImageModel ): \"\"\" Predicts the type of incident based on the provided image and other data. Args: data (ImageModel): The input data containing image name, sensitive structures, and incident ID. Returns: JSONResponse: The prediction results including incident type, probabilities, context, impact, and solution. Raises: HTTPException: If any step in the prediction process fails. \"\"\" try : logger . info ( f \"Received request for image: { data . image_name } with sensitive structures: { data . sensitive_structures } and incident_id: { data . incident_id } \" ) image_url = construct_image_url ( data . image_name ) image = await fetch_image ( image_url ) # Perform prediction asynchronously using Celery prediction_task = perform_prediction . delay ( image ) try : prediction , probabilities = prediction_task . get ( timeout = 120 ) logger . info ( f \"Prediction successful: { prediction } with probabilities: { probabilities } \" ) if isinstance ( probabilities , np . ndarray ): probabilities = probabilities . tolist () except Exception as e : logger . error ( f \"Error during prediction task: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Error during prediction: { str ( e ) } \" ) # Fetch contextual information asynchronously using Celery context_task = fetch_contextual_information . delay ( prediction , data . sensitive_structures ) try : get_context , impact , piste_solution = context_task . get ( timeout = 120 ) logger . info ( f \"Context fetching successful: { get_context } , { impact } , { piste_solution } \" ) except Exception as e : logger . error ( f \"Error during context fetching task: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Error during context fetching: { str ( e ) } \" ) # Validate all required fields are present if not all ([ data . incident_id , prediction , piste_solution , impact , get_context ]): raise HTTPException ( status_code = 400 , detail = \"Missing required fields for database insertion.\" ) # Insert the prediction and context into the database query = \"\"\" INSERT INTO \"Mapapi_prediction\" (incident_id, incident_type, piste_solution, impact_potentiel, context) VALUES (:incident_id, :incident_type, :piste_solution, :impact_potentiel, :context); \"\"\" values = { \"incident_id\" : data . incident_id , \"incident_type\" : prediction , \"piste_solution\" : piste_solution , \"impact_potentiel\" : impact , \"context\" : get_context , } try : await database . execute ( query = query , values = values ) logger . info ( \"Database insertion successful\" ) except Exception as e : logger . error ( f \"Database error: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Database error: { str ( e ) } \" ) return JSONResponse ( content = { \"prediction\" : prediction , \"probabilities\" : probabilities , \"context\" : get_context , \"in_depth\" : impact , \"piste_solution\" : piste_solution , } ) except HTTPException as http_exc : # Re-raise HTTPExceptions to be handled by the global exception handler raise http_exc except Exception as e : logger . error ( f \"Unhandled exception: { e } \" ) raise HTTPException ( status_code = 500 , detail = str ( e ))","title":"predict_incident_type"},{"location":"api_route/#apis.main_router.sanitize_error_message","text":"Sanitizes the error message by masking sensitive structures. Parameters: Name Type Description Default message str The original error message. required sensitive_structures List [ str ] A list of sensitive terms to mask. required Returns: Name Type Description str str The sanitized error message. Source code in app/apis/main_router.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def sanitize_error_message ( message : str , sensitive_structures : List [ str ]) -> str : \"\"\" Sanitizes the error message by masking sensitive structures. Args: message (str): The original error message. sensitive_structures (List[str]): A list of sensitive terms to mask. Returns: str: The sanitized error message. \"\"\" sanitized_message = message for structure in sensitive_structures : sanitized_message = sanitized_message . replace ( structure , \"***\" ) return sanitized_message","title":"sanitize_error_message"},{"location":"api_route/#apis.main_router.save_chat_history","text":"Saves a single chat interaction to the database. Parameters: Name Type Description Default chat_key str The unique identifier for the chat session. required question str The user's question. required answer str The assistant's answer. required Raises: Type Description HTTPException If there is an error saving to the database. Source code in app/apis/main_router.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 async def save_chat_history ( chat_key : str , question : str , answer : str ): \"\"\" Saves a single chat interaction to the database. Args: chat_key (str): The unique identifier for the chat session. question (str): The user's question. answer (str): The assistant's answer. Raises: HTTPException: If there is an error saving to the database. \"\"\" query = \"\"\" INSERT INTO \"Mapapi_chathistory\" (session_id, question, answer) VALUES (:session_id, :question, :answer); \"\"\" values = { \"session_id\" : chat_key , \"question\" : question , \"answer\" : answer , } try : await database . execute ( query = query , values = values ) logger . info ( f \"Chat history saved for session { chat_key } \" ) except Exception as e : logger . error ( f \"Error saving chat history: { e } \" )","title":"save_chat_history"},{"location":"celery/","text":"make_celery () Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Name Type Description Celery A Celery application instance configured to use Redis for queuing tasks and storing results. Source code in app/services/celery/celery_config.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def make_celery (): \"\"\" Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Celery: A Celery application instance configured to use Redis for queuing tasks and storing results. \"\"\" # Retrieve Redis connection details from environment variables redis_host = os . getenv ( 'REDIS_HOST' , 'redis' ) redis_port = os . getenv ( 'REDIS_PORT' , 6379 ) redis_username = os . getenv ( 'REDIS_USERNAME' , '' ) redis_password = os . getenv ( 'REDIS_PASSWORD' , '' ) redis_url = os . getenv ( 'REDIS_URL' ) # Fetch the Redis URL from the environment variable celery = Celery ( 'worker' , # Name of the worker backend = redis_url , # Use Redis URL from environment broker = redis_url # Use Redis URL from environment ) return celery fetch_contextual_information ( prediction , sensitive_structures ) A Celery task that fetches contextual information based on the prediction and sensitive structures. Parameters: Name Type Description Default prediction str The predicted classification. required sensitive_structures list A list of sensitive structures related to the prediction. required Returns: Name Type Description tuple A tuple containing context, impact, and piste_solution. Source code in app/services/celery/celery_task.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @celery_app . task def fetch_contextual_information ( prediction , sensitive_structures ): \"\"\" A Celery task that fetches contextual information based on the prediction and sensitive structures. Args: prediction (str): The predicted classification. sensitive_structures (list): A list of sensitive structures related to the prediction. Returns: tuple: A tuple containing context, impact, and piste_solution. \"\"\" try : logger . info ( \"Starting contextual information task.\" ) context_prompt = f \"Vous \u00eates un expert en environnement. Expliquez le contexte et l'importance de { prediction } au Mali en mettant en \u00e9vidence les diff\u00e9rents types de terrains et leurs caract\u00e9ristiques sp\u00e9cifiques.\" impact_prompt = f \"En tant qu'expert en environnement, d\u00e9crivez les impacts potentiels de { prediction } sur les structures sensibles comme { sensitive_structures } au Mali.\" solution_prompt = f \"En tant qu'expert en gestion environnementale, proposez des solutions pratiques et durables pour g\u00e9rer les impacts de { prediction } au Mali. Incluez des mesures pr\u00e9ventives et correctives.\" # Fetching responses from the model get_context = get_response ( context_prompt ) impact = get_response ( impact_prompt ) piste_solution = get_response ( solution_prompt ) logger . info ( f \"Context: { get_context } , Impact: { impact } , Solution: { piste_solution } \" ) return get_context , impact , piste_solution except Exception as e : logger . error ( f \"Contextual information task failed: { str ( e ) } \" ) return { \"error\" : str ( e )}, None , None perform_prediction ( image ) A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Parameters: Name Type Description Default image bytes The image data in bytes format, ready to be processed by the prediction model. required Returns: Name Type Description tuple A tuple containing the predicted classification and a list of probabilities associated with each class. Source code in app/services/celery/celery_task.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @celery_app . task def perform_prediction ( image ): \"\"\" A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Args: image (bytes): The image data in bytes format, ready to be processed by the prediction model. Returns: tuple: A tuple containing the predicted classification and a list of probabilities associated with each class. \"\"\" try : logger . info ( \"Starting prediction task.\" ) prediction , probabilities = predict ( image ) logger . info ( f \"Prediction: { prediction } , Probabilities: { probabilities } \" ) # Ensure `probabilities` is a list if isinstance ( probabilities , list ): return prediction , probabilities else : return prediction , probabilities . tolist () except Exception as e : logger . error ( f \"Prediction task failed: { str ( e ) } \" ) return { \"error\" : str ( e )}, [] run_prediction_and_context ( image , sensitive_structures ) Chain the prediction task with the contextual information task. Source code in app/services/celery/celery_task.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 @celery_app . task def run_prediction_and_context ( image , sensitive_structures ): \"\"\" Chain the prediction task with the contextual information task. \"\"\" try : logger . info ( \"Starting chained task: run_prediction_and_context.\" ) prediction , probabilities = perform_prediction ( image ) # Proceed with fetching contextual information only if prediction is successful if prediction and not isinstance ( prediction , dict ): # Ensure it's not an error dict context_info = fetch_contextual_information . delay ( prediction , sensitive_structures ) return context_info . get ( timeout = 120 ) else : logger . error ( f \"Failed to proceed due to prediction error: { prediction } \" ) return { \"error\" : \"Prediction failed, unable to fetch contextual information.\" } except Exception as e : logger . error ( f \"Chained task failed: { e } \" ) return { \"error\" : str ( e )}","title":"celery config and tasks"},{"location":"celery/#services.celery.celery_config.make_celery","text":"Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Name Type Description Celery A Celery application instance configured to use Redis for queuing tasks and storing results. Source code in app/services/celery/celery_config.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def make_celery (): \"\"\" Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Celery: A Celery application instance configured to use Redis for queuing tasks and storing results. \"\"\" # Retrieve Redis connection details from environment variables redis_host = os . getenv ( 'REDIS_HOST' , 'redis' ) redis_port = os . getenv ( 'REDIS_PORT' , 6379 ) redis_username = os . getenv ( 'REDIS_USERNAME' , '' ) redis_password = os . getenv ( 'REDIS_PASSWORD' , '' ) redis_url = os . getenv ( 'REDIS_URL' ) # Fetch the Redis URL from the environment variable celery = Celery ( 'worker' , # Name of the worker backend = redis_url , # Use Redis URL from environment broker = redis_url # Use Redis URL from environment ) return celery","title":"make_celery"},{"location":"celery/#services.celery.celery_task.fetch_contextual_information","text":"A Celery task that fetches contextual information based on the prediction and sensitive structures. Parameters: Name Type Description Default prediction str The predicted classification. required sensitive_structures list A list of sensitive structures related to the prediction. required Returns: Name Type Description tuple A tuple containing context, impact, and piste_solution. Source code in app/services/celery/celery_task.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @celery_app . task def fetch_contextual_information ( prediction , sensitive_structures ): \"\"\" A Celery task that fetches contextual information based on the prediction and sensitive structures. Args: prediction (str): The predicted classification. sensitive_structures (list): A list of sensitive structures related to the prediction. Returns: tuple: A tuple containing context, impact, and piste_solution. \"\"\" try : logger . info ( \"Starting contextual information task.\" ) context_prompt = f \"Vous \u00eates un expert en environnement. Expliquez le contexte et l'importance de { prediction } au Mali en mettant en \u00e9vidence les diff\u00e9rents types de terrains et leurs caract\u00e9ristiques sp\u00e9cifiques.\" impact_prompt = f \"En tant qu'expert en environnement, d\u00e9crivez les impacts potentiels de { prediction } sur les structures sensibles comme { sensitive_structures } au Mali.\" solution_prompt = f \"En tant qu'expert en gestion environnementale, proposez des solutions pratiques et durables pour g\u00e9rer les impacts de { prediction } au Mali. Incluez des mesures pr\u00e9ventives et correctives.\" # Fetching responses from the model get_context = get_response ( context_prompt ) impact = get_response ( impact_prompt ) piste_solution = get_response ( solution_prompt ) logger . info ( f \"Context: { get_context } , Impact: { impact } , Solution: { piste_solution } \" ) return get_context , impact , piste_solution except Exception as e : logger . error ( f \"Contextual information task failed: { str ( e ) } \" ) return { \"error\" : str ( e )}, None , None","title":"fetch_contextual_information"},{"location":"celery/#services.celery.celery_task.perform_prediction","text":"A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Parameters: Name Type Description Default image bytes The image data in bytes format, ready to be processed by the prediction model. required Returns: Name Type Description tuple A tuple containing the predicted classification and a list of probabilities associated with each class. Source code in app/services/celery/celery_task.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @celery_app . task def perform_prediction ( image ): \"\"\" A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Args: image (bytes): The image data in bytes format, ready to be processed by the prediction model. Returns: tuple: A tuple containing the predicted classification and a list of probabilities associated with each class. \"\"\" try : logger . info ( \"Starting prediction task.\" ) prediction , probabilities = predict ( image ) logger . info ( f \"Prediction: { prediction } , Probabilities: { probabilities } \" ) # Ensure `probabilities` is a list if isinstance ( probabilities , list ): return prediction , probabilities else : return prediction , probabilities . tolist () except Exception as e : logger . error ( f \"Prediction task failed: { str ( e ) } \" ) return { \"error\" : str ( e )}, []","title":"perform_prediction"},{"location":"celery/#services.celery.celery_task.run_prediction_and_context","text":"Chain the prediction task with the contextual information task. Source code in app/services/celery/celery_task.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 @celery_app . task def run_prediction_and_context ( image , sensitive_structures ): \"\"\" Chain the prediction task with the contextual information task. \"\"\" try : logger . info ( \"Starting chained task: run_prediction_and_context.\" ) prediction , probabilities = perform_prediction ( image ) # Proceed with fetching contextual information only if prediction is successful if prediction and not isinstance ( prediction , dict ): # Ensure it's not an error dict context_info = fetch_contextual_information . delay ( prediction , sensitive_structures ) return context_info . get ( timeout = 120 ) else : logger . error ( f \"Failed to proceed due to prediction error: { prediction } \" ) return { \"error\" : \"Prediction failed, unable to fetch contextual information.\" } except Exception as e : logger . error ( f \"Chained task failed: { e } \" ) return { \"error\" : str ( e )}","title":"run_prediction_and_context"},{"location":"cnn/","text":"","title":"cnn service"},{"location":"contribution-guideline/","text":"Map Action Contribution Guidelines Welcome to Map Action! We're excited about your interest in contributing to our open-source project. These guidelines will help you understand how to effectively contribute to our codebase. About Map Action Map Action is a Bamako-based trailblazer in using mapping technology to tackle environmental challenges and solve urban issues. Our innovative approach began by addressing Water, Sanitation, and Hygiene (WASH) problems and has grown to encompass various sectors through collaboration with civil society, governments, NGOs, and private entities. Our sustainable business model includes paid subscriptions for organizations that rely on our data to gain actionable insights across diverse domains. This model allows us to continuously enhance our offerings. Contributing We welcome contributions of all shapes and sizes! Here are the different ways you can get involved: Report bugs and request features: Identify any issues you encounter while using our tools. You can report them directly on GitHub Issues https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues. Submit code changes: If you have improvements or new features in mind, you can contribute code by creating pull requests (PRs). Write documentation or tutorials: Enhance our documentation to make it easier for others to understand and use our tools. Help with code reviews and testing: Lend your expertise by reviewing pull requests submitted by others and assisting with testing efforts. Finding Issues to Work On Browse our issue tracker on GitHub Issues https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues to find existing issues. Look for issues labeled \"help wanted\" or those categorized as bugs or enhancements that you're interested in tackling. Making Changes Fork the Repository: Visit the Map Action project on GitHub: https://github.com/223MapAction Click on the \"Fork\" button to create your own copy of the repository. Create a Branch: Clone your forked repository to your local machine. Create a new branch for your specific changes. Use a descriptive branch name that reflects your contribution (e.g., \"fix-map-loading-bug\"). Code and Commit: Make your changes to the codebase. Write clear and concise commit messages that describe your modifications. Testing: Ensure your changes don't introduce any regressions. We use pytest and flake8 for testing. Make sure your code passes all tests before submitting a pull request. Submitting Pull Requests Push to Your Branch: Once you're satisfied with your changes, push your local branch to your forked repository on GitHub. Open a Pull Request: Navigate to your forked repository on GitHub and go to the \"Pull Requests\" tab. Click on \"New pull request\" and select the branch containing your changes. Create a pull request with a clear and descriptive title and explanation of your modifications. Mention any issues your pull request addresses. Code Review: Our internal code review process involves two Map Action developers. They'll review your pull request and provide feedback or suggestions for improvement. Merge: Once your pull request is approved and any necessary changes are made, it will be merged into the main codebase. Additional Notes License: All our repositories use the GPL-3.0 license. Ensure your contributions comply with the license terms. Code of Conduct: We value a respectful and inclusive environment. Please familiarize yourself with our [Code of Conduct](CODE_OF_CONDUCT.md) before contributing. Appreciation: We appreciate all contributions, regardless of their scope. Thank you for helping us improve Map Action! We look forward to your contributions! This tailored guide incorporates the information you provided, making it specific to the Map Action project and its contribution workflow.","title":"contrubition guideline"},{"location":"install-run/","text":"Getting Started System Requirements: Python : 3.x Installation From source Clone the ML-Deploy repository: $ git clone https://github.com/223MapAction/ML-Deploy.git Change to the project directory: $ cd ML-Deploy Create a virtual environement: $ python3 -m venv env Install the dependencies: $ pip install -r requirements.txt Usage From source Run ML-Deploy using the command below: $ uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload Tests Run the test suite using the command below: console $ pytest --cov=app --cov-report term-missing","title":"Install and run"},{"location":"install-run/#getting-started","text":"System Requirements: Python : 3.x","title":"Getting Started"},{"location":"install-run/#installation","text":"","title":"Installation"},{"location":"install-run/#usage","text":"","title":"Usage"},{"location":"install-run/#tests","text":"Run the test suite using the command below: console $ pytest --cov=app --cov-report term-missing","title":"Tests"},{"location":"llm/","text":"chat_response ( prompt , context = '' , chat_history = []) Processes a user's prompt to generate the assistant's response using GPT-4o-mini, with context about the environmental incident. Parameters: Name Type Description Default prompt str The user's message to which the assistant should respond. required context str A JSON string containing context about the incident. '' chat_history list The existing chat history for this session. [] Returns: Name Type Description str The assistant's response. Source code in app/services/llm/gpt_3_5_turbo.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def chat_response ( prompt : str , context : str = \"\" , chat_history : list = []): \"\"\" Processes a user's prompt to generate the assistant's response using GPT-4o-mini, with context about the environmental incident. Args: prompt (str): The user's message to which the assistant should respond. context (str): A JSON string containing context about the incident. chat_history (list): The existing chat history for this session. Returns: str: The assistant's response. \"\"\" context_obj = json . loads ( context ) system_message = f \"\"\" Vous \u00eates un assistant AI sp\u00e9cialis\u00e9 dans l'analyse des incidents environnementaux au Mali. Voici le contexte d\u00e9taill\u00e9 de l'incident actuel: Type d'incident: { context_obj [ 'type_incident' ] } Contexte: { context_obj [ 'context' ] } Impact potentiel: { context_obj [ 'impact_potentiel' ] } Pistes de solution: { context_obj [ 'piste_solution' ] } Votre t\u00e2che est de fournir des informations pr\u00e9cises et pertinentes en fran\u00e7ais, bas\u00e9es sur ce contexte d\u00e9taill\u00e9. Adaptez vos r\u00e9ponses au contexte sp\u00e9cifique de l'incident et utilisez ces informations pour enrichir vos explications. Si on vous pose des questions sur des aspects non couverts par ce contexte, vous pouvez y r\u00e9pondre en vous basant sur vos connaissances g\u00e9n\u00e9rales, mais pr\u00e9cisez que ces informations ne font pas partie du rapport sp\u00e9cifique de cet incident. \"\"\" messages = [{ \"role\" : \"system\" , \"content\" : system_message }] + chat_history + [{ \"role\" : \"user\" , \"content\" : prompt }] try : # Get the assistant's response response = client . chat . completions . create ( model = \"gpt-4o-mini\" , # Make sure this model is available and correct messages = messages , temperature = 1 , max_tokens = 1080 , top_p = 1 , frequency_penalty = 0 , presence_penalty = 0 ) assistant_response = response . choices [ 0 ] . message . content return assistant_response except Exception as e : print ( f \"An error occurred: { e } \" ) return \"Sorry, I can't process your request right now.\" display_chat_history ( messages ) Prints the chat history to the console. Each message is displayed with the sender's role and content. Parameters: Name Type Description Default messages list of dict A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. required Source code in app/services/llm/gpt_3_5_turbo.py 13 14 15 16 17 18 19 20 21 22 def display_chat_history ( messages ): \"\"\" Prints the chat history to the console. Each message is displayed with the sender's role and content. Args: messages (list of dict): A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. \"\"\" for message in messages : print ( f \" { message [ 'role' ] . capitalize () } : { message [ 'content' ] } \" ) get_assistant_response ( messages ) Sends the current chat history to the OpenAI API to generate a response from the assistant using GPT-4o-mini. Parameters: Name Type Description Default messages list of dict The current chat history as a list of message dictionaries. required Returns: Name Type Description str The assistant's response as a string. Raises: Type Description Exception Prints an error message if the API call fails and returns a default error response. Source code in app/services/llm/gpt_3_5_turbo.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_assistant_response ( messages ): \"\"\" Sends the current chat history to the OpenAI API to generate a response from the assistant using GPT-4o-mini. Args: messages (list of dict): The current chat history as a list of message dictionaries. Returns: str: The assistant's response as a string. Raises: Exception: Prints an error message if the API call fails and returns a default error response. \"\"\" try : r = client . chat . completions . create ( model = \"gpt-4o-mini\" , # The model version to use for generating responses messages = [{ \"role\" : m [ \"role\" ], \"content\" : m [ \"content\" ]} for m in messages ], temperature = 1 , # Adjust the temperature if needed max_tokens = 1080 , # Adjust as needed top_p = 1 , frequency_penalty = 0 , presence_penalty = 0 ) response = r . choices [ 0 ] . message . content return response except Exception as e : print ( f \"An error occurred: { e } \" ) return \"Sorry, I can't process your request right now.\" get_response ( prompt ) Processes a user's prompt to generate and display the assistant's response using GPT-4o-mini. Parameters: Name Type Description Default prompt str The user's message to which the assistant should respond. required Returns: Name Type Description str The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. Source code in app/services/llm/gpt_3_5_turbo.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_response ( prompt : str ): \"\"\" Processes a user's prompt to generate and display the assistant's response using GPT-4o-mini. Args: prompt (str): The user's message to which the assistant should respond. Returns: str: The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. \"\"\" # Add the user's message to the chat history messages . append ({ \"role\" : \"user\" , \"content\" : prompt }) # Get the assistant's response and add it to the chat history response = get_assistant_response ( messages ) messages . append ({ \"role\" : \"assistant\" , \"content\" : response }) # Display the updated chat history display_chat_history ( messages ) return response","title":"llm service"},{"location":"llm/#services.llm.gpt_3_5_turbo.chat_response","text":"Processes a user's prompt to generate the assistant's response using GPT-4o-mini, with context about the environmental incident. Parameters: Name Type Description Default prompt str The user's message to which the assistant should respond. required context str A JSON string containing context about the incident. '' chat_history list The existing chat history for this session. [] Returns: Name Type Description str The assistant's response. Source code in app/services/llm/gpt_3_5_turbo.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def chat_response ( prompt : str , context : str = \"\" , chat_history : list = []): \"\"\" Processes a user's prompt to generate the assistant's response using GPT-4o-mini, with context about the environmental incident. Args: prompt (str): The user's message to which the assistant should respond. context (str): A JSON string containing context about the incident. chat_history (list): The existing chat history for this session. Returns: str: The assistant's response. \"\"\" context_obj = json . loads ( context ) system_message = f \"\"\" Vous \u00eates un assistant AI sp\u00e9cialis\u00e9 dans l'analyse des incidents environnementaux au Mali. Voici le contexte d\u00e9taill\u00e9 de l'incident actuel: Type d'incident: { context_obj [ 'type_incident' ] } Contexte: { context_obj [ 'context' ] } Impact potentiel: { context_obj [ 'impact_potentiel' ] } Pistes de solution: { context_obj [ 'piste_solution' ] } Votre t\u00e2che est de fournir des informations pr\u00e9cises et pertinentes en fran\u00e7ais, bas\u00e9es sur ce contexte d\u00e9taill\u00e9. Adaptez vos r\u00e9ponses au contexte sp\u00e9cifique de l'incident et utilisez ces informations pour enrichir vos explications. Si on vous pose des questions sur des aspects non couverts par ce contexte, vous pouvez y r\u00e9pondre en vous basant sur vos connaissances g\u00e9n\u00e9rales, mais pr\u00e9cisez que ces informations ne font pas partie du rapport sp\u00e9cifique de cet incident. \"\"\" messages = [{ \"role\" : \"system\" , \"content\" : system_message }] + chat_history + [{ \"role\" : \"user\" , \"content\" : prompt }] try : # Get the assistant's response response = client . chat . completions . create ( model = \"gpt-4o-mini\" , # Make sure this model is available and correct messages = messages , temperature = 1 , max_tokens = 1080 , top_p = 1 , frequency_penalty = 0 , presence_penalty = 0 ) assistant_response = response . choices [ 0 ] . message . content return assistant_response except Exception as e : print ( f \"An error occurred: { e } \" ) return \"Sorry, I can't process your request right now.\"","title":"chat_response"},{"location":"llm/#services.llm.gpt_3_5_turbo.display_chat_history","text":"Prints the chat history to the console. Each message is displayed with the sender's role and content. Parameters: Name Type Description Default messages list of dict A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. required Source code in app/services/llm/gpt_3_5_turbo.py 13 14 15 16 17 18 19 20 21 22 def display_chat_history ( messages ): \"\"\" Prints the chat history to the console. Each message is displayed with the sender's role and content. Args: messages (list of dict): A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. \"\"\" for message in messages : print ( f \" { message [ 'role' ] . capitalize () } : { message [ 'content' ] } \" )","title":"display_chat_history"},{"location":"llm/#services.llm.gpt_3_5_turbo.get_assistant_response","text":"Sends the current chat history to the OpenAI API to generate a response from the assistant using GPT-4o-mini. Parameters: Name Type Description Default messages list of dict The current chat history as a list of message dictionaries. required Returns: Name Type Description str The assistant's response as a string. Raises: Type Description Exception Prints an error message if the API call fails and returns a default error response. Source code in app/services/llm/gpt_3_5_turbo.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_assistant_response ( messages ): \"\"\" Sends the current chat history to the OpenAI API to generate a response from the assistant using GPT-4o-mini. Args: messages (list of dict): The current chat history as a list of message dictionaries. Returns: str: The assistant's response as a string. Raises: Exception: Prints an error message if the API call fails and returns a default error response. \"\"\" try : r = client . chat . completions . create ( model = \"gpt-4o-mini\" , # The model version to use for generating responses messages = [{ \"role\" : m [ \"role\" ], \"content\" : m [ \"content\" ]} for m in messages ], temperature = 1 , # Adjust the temperature if needed max_tokens = 1080 , # Adjust as needed top_p = 1 , frequency_penalty = 0 , presence_penalty = 0 ) response = r . choices [ 0 ] . message . content return response except Exception as e : print ( f \"An error occurred: { e } \" ) return \"Sorry, I can't process your request right now.\"","title":"get_assistant_response"},{"location":"llm/#services.llm.gpt_3_5_turbo.get_response","text":"Processes a user's prompt to generate and display the assistant's response using GPT-4o-mini. Parameters: Name Type Description Default prompt str The user's message to which the assistant should respond. required Returns: Name Type Description str The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. Source code in app/services/llm/gpt_3_5_turbo.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_response ( prompt : str ): \"\"\" Processes a user's prompt to generate and display the assistant's response using GPT-4o-mini. Args: prompt (str): The user's message to which the assistant should respond. Returns: str: The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. \"\"\" # Add the user's message to the chat history messages . append ({ \"role\" : \"user\" , \"content\" : prompt }) # Get the assistant's response and add it to the chat history response = get_assistant_response ( messages ) messages . append ({ \"role\" : \"assistant\" , \"content\" : response }) # Display the updated chat history display_chat_history ( messages ) return response","title":"get_response"},{"location":"models/","text":"ImageModel Bases: BaseModel A model representing image data for processing and prediction in the API. Attributes: Name Type Description image_name str The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures List [ str ] A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id str A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. Source code in app/models/image_model.py 4 5 6 7 8 9 10 11 12 13 14 15 16 class ImageModel ( BaseModel ): \"\"\" A model representing image data for processing and prediction in the API. Attributes: image_name (str): The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures (List[str]): A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id (str): A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. \"\"\" #Id: int # Uncomment if an ID attribute is necessary image_name : str sensitive_structures : List [ str ] incident_id : str","title":"Pydantic model"},{"location":"models/#models.image_model.ImageModel","text":"Bases: BaseModel A model representing image data for processing and prediction in the API. Attributes: Name Type Description image_name str The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures List [ str ] A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id str A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. Source code in app/models/image_model.py 4 5 6 7 8 9 10 11 12 13 14 15 16 class ImageModel ( BaseModel ): \"\"\" A model representing image data for processing and prediction in the API. Attributes: image_name (str): The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures (List[str]): A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id (str): A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. \"\"\" #Id: int # Uncomment if an ID attribute is necessary image_name : str sensitive_structures : List [ str ] incident_id : str","title":"ImageModel"},{"location":"system-arch/","text":"","title":"System Arch"}]}