{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hey! it's Map Action Map Action envisions a world where technology revolutionizes environmental management and rural & urban problem-solving, making sustainable living accessible to all communities in Mali and beyond. In a world increasingly challenged by environmental issues and urban complexities, Map Action envisions a future where cutting-edge technology and geospatial solutions empower communities, governments, and organizations. Our vision is to create a global society where sustainable urban and rural development and environmental stewardship are not only achievable but are the cornerstones of our collective well-being. Mission Statement Our mission is to develop, deploy, and promote open-source mapping tools and methodologies that enable individuals, communities, governments, and organizations to collaboratively identify, analyze, and solve critical environmental and urban challenges. Community Statement Map Action thrives on the strength of a diverse, inclusive community united by the goal of using technology for sustainable urban and environmental management. We commit to fostering an open, respectful environment where every voice is valued, and collaboration drives innovation. Together, we empower individuals and organizations to actively participate in crafting solutions that make a meaningful impact.","title":"Welcom"},{"location":"#hey-its-map-action","text":"Map Action envisions a world where technology revolutionizes environmental management and rural & urban problem-solving, making sustainable living accessible to all communities in Mali and beyond. In a world increasingly challenged by environmental issues and urban complexities, Map Action envisions a future where cutting-edge technology and geospatial solutions empower communities, governments, and organizations. Our vision is to create a global society where sustainable urban and rural development and environmental stewardship are not only achievable but are the cornerstones of our collective well-being.","title":"Hey! it's Map Action"},{"location":"api_route/","text":"chat_endpoint ( websocket ) async WebSocket endpoint for handling chat interactions and chat history deletion. Parameters: Name Type Description Default websocket WebSocket The WebSocket connection. required Raises: Type Description WebSocketDisconnect If the connection is closed. Source code in app/apis/main_router.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 @router . websocket ( \"/ws/chat\" ) async def chat_endpoint ( websocket : WebSocket ): \"\"\" WebSocket endpoint for handling chat interactions and chat history deletion. Args: websocket (WebSocket): The WebSocket connection. Raises: WebSocketDisconnect: If the connection is closed. \"\"\" logger . info ( f \"WebSocket connection attempt from { websocket . client . host } \" ) origin = websocket . headers . get ( \"origin\" ) logger . info ( f \"Received Origin: { origin } \" ) allowed_origins = [ \"http://197.155.176.134\" , \"http://localhost:3000\" , \"http://127.0.0.1\" , \"http://57.153.185.160\" , \"https://app.map-action.com\" , \"http://app.map-action.com\" , None , ] if origin not in allowed_origins : logger . warning ( f \"Connection rejected from origin: { origin } \" ) await websocket . close ( code = status . WS_1008_POLICY_VIOLATION ) return logger . info ( f \"Connection accepted from origin: { origin } \" ) await manager . connect ( websocket ) try : while True : data = await websocket . receive_json () action = data . get ( \"action\" ) incident_id = data . get ( \"incident_id\" ) session_id = data . get ( \"session_id\" ) if not incident_id or not session_id : logger . error ( \"Missing incident_id or session_id in message\" ) await websocket . send_json ({ \"error\" : \"Missing incident_id or session_id\" }) continue if action == \"delete_chat\" : # Handle chat history deletion chat_key = f \" { session_id }{ incident_id } \" query = \"\"\" DELETE FROM \"Mapapi_chathistory\" WHERE session_id = :session_id; \"\"\" values = { \"session_id\" : chat_key } try : await database . execute ( query = query , values = values ) logger . info ( f \"Chat history deleted for session { chat_key } \" ) # Clear in-memory chat history if chat_key in chat_histories : del chat_histories [ chat_key ] # Send a confirmation to the client await websocket . send_json ({ \"message\" : \"Chat history deleted successfully.\" }) except Exception as e : logger . error ( f \"Error deleting chat history: { e } \" ) await websocket . send_json ({ \"error\" : \"Error deleting chat history.\" }) else : # Fetch context from the database based on incident_id query = \"\"\" SELECT incident_type, analysis, piste_solution FROM \"Mapapi_prediction\" WHERE incident_id = :incident_id; \"\"\" values = { \"incident_id\" : incident_id } result = await database . fetch_one ( query = query , values = values ) if result : context_obj = { \"type_incident\" : result [ \"incident_type\" ], \"analysis\" : result [ \"analysis\" ], \"piste_solution\" : result [ \"piste_solution\" ], } context = json . dumps ( context_obj ) # Retrieve impact_area from in-memory storage impact_area = impact_area_storage . get ( incident_id , None ) else : logger . error ( f \"No context found for incident_id: { incident_id } \" ) await websocket . send_json ({ \"error\" : \"No context found for the given incident_id\" }) await websocket . close ( code = status . WS_1008_POLICY_VIOLATION ) return chat_key = f \" { session_id }{ incident_id } \" # Initialize chat history if not present if chat_key not in chat_histories : history_query = \"\"\" SELECT question, answer FROM \"Mapapi_chathistory\" WHERE session_id = :session_id ORDER BY id ASC; \"\"\" history_values = { \"session_id\" : chat_key } history_results = await database . fetch_all ( query = history_query , values = history_values ) chat_histories [ chat_key ] = [ { \"role\" : \"user\" , \"content\" : record [ \"question\" ]} for record in history_results ] + [ { \"role\" : \"assistant\" , \"content\" : record [ \"answer\" ]} for record in history_results ] # Add the user's question to the chat history question = data . get ( \"question\" ) chat_histories [ chat_key ] . append ({ \"role\" : \"user\" , \"content\" : question }) # Get response from chat bot chatbot_response = chat_response ( question , context , chat_histories [ chat_key ], impact_area ) # Append assistant's response to history chat_histories [ chat_key ] . append ({ \"role\" : \"assistant\" , \"content\" : chatbot_response }) # Send the response back through the WebSocket response_message = { \"incident_id\" : incident_id , \"session_id\" : session_id , \"question\" : question , \"answer\" : chatbot_response , } await websocket . send_json ( response_message ) # Save the chat history to the database await save_chat_history ( chat_key , question , chatbot_response ) except WebSocketDisconnect : manager . disconnect ( websocket ) logger . info ( f \"WebSocket disconnected from { websocket . client . host } \" ) except Exception as e : logger . error ( f \"WebSocket error: { e } \" ) await websocket . close ( code = status . WS_1011_INTERNAL_ERROR ) construct_image_url ( image_name ) Constructs the full URL for the image based on the image name. Parameters: Name Type Description Default image_name str The name or path of the image. required Returns: Name Type Description str str The full URL to access the image. Source code in app/apis/main_router.py 41 42 43 44 45 46 47 48 49 50 51 def construct_image_url ( image_name : str ) -> str : \"\"\" Constructs the full URL for the image based on the image name. Args: image_name (str): The name or path of the image. Returns: str: The full URL to access the image. \"\"\" return f \" { BASE_URL } / { image_name . split ( '/' )[ - 1 ] } \" fetch_image ( image_url ) async Fetches the image from the specified URL. Parameters: Name Type Description Default image_url str The URL of the image to fetch. required Returns: Name Type Description bytes bytes The binary content of the fetched image. Raises: Type Description HTTPException If the image cannot be fetched. Source code in app/apis/main_router.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 async def fetch_image ( image_url : str ) -> bytes : \"\"\" Fetches the image from the specified URL. Args: image_url (str): The URL of the image to fetch. Returns: bytes: The binary content of the fetched image. Raises: HTTPException: If the image cannot be fetched. \"\"\" try : response = requests . get ( image_url ) response . raise_for_status () return response . content except requests . RequestException as e : logger . error ( f \"Failed to fetch image from { image_url } : { str ( e ) } \" ) raise HTTPException ( status_code = 500 , detail = f \"Failed to fetch image: { str ( e ) } \" ) get_chat_history ( chat_key ) async Retrieves the chat history for a given chat_key. Parameters: Name Type Description Default chat_key str The unique identifier for the chat session. required Returns: Name Type Description list A list of chat messages in chronological order. Raises: Type Description HTTPException If there is an error fetching the chat history. Source code in app/apis/main_router.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 @router . get ( \"/MapApi/history/ {chat_key} \" ) async def get_chat_history ( chat_key : str ): \"\"\" Retrieves the chat history for a given chat_key. Args: chat_key (str): The unique identifier for the chat session. Returns: list: A list of chat messages in chronological order. Raises: HTTPException: If there is an error fetching the chat history. \"\"\" query = \"\"\" SELECT question, answer FROM \"Mapapi_chathistory\" WHERE session_id = :session_id ORDER BY id ASC; \"\"\" values = { \"session_id\" : chat_key } try : results = await database . fetch_all ( query = query , values = values ) # Format the results to interleave user and assistant messages formatted_history = [] for record in results : formatted_history . append ({ \"role\" : \"user\" , \"content\" : record [ \"question\" ]}) formatted_history . append ({ \"role\" : \"assistant\" , \"content\" : record [ \"answer\" ]}) return formatted_history except Exception as e : logger . error ( f \"Error fetching chat history: { e } \" ) raise HTTPException ( status_code = 500 , detail = \"Error fetching chat history\" ) index () Root endpoint to verify that the API is running. Returns: Name Type Description dict A message indicating the API is operational. Source code in app/apis/main_router.py 90 91 92 93 94 95 96 97 98 @router . get ( \"/\" ) def index (): \"\"\" Root endpoint to verify that the API is running. Returns: dict: A message indicating the API is operational. \"\"\" return { \"message\" : \"Map Action classification model\" } predict_incident_type ( data ) async Predicts the type of incident based on the provided image and other data. Parameters: Name Type Description Default data ImageModel The input data containing image name, sensitive structures, zone, and incident ID. required Returns: Name Type Description JSONResponse The prediction results including incident type, probabilities, context, impact, and solution. Raises: Type Description HTTPException If any step in the prediction process fails. Source code in app/apis/main_router.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @router . post ( \"/image/predict\" ) async def predict_incident_type ( data : ImageModel ): \"\"\" Predicts the type of incident based on the provided image and other data. Args: data (ImageModel): The input data containing image name, sensitive structures, zone, and incident ID. Returns: JSONResponse: The prediction results including incident type, probabilities, context, impact, and solution. Raises: HTTPException: If any step in the prediction process fails. \"\"\" try : logger . info ( f \"Received request for image: { data . image_name } with sensitive structures: { data . sensitive_structures } , incident_id: { data . incident_id } in zone: { data . zone } \" ) image_url = construct_image_url ( data . image_name ) image = await fetch_image ( image_url ) # Perform prediction asynchronously using Celery prediction_task = perform_prediction . delay ( image ) try : prediction , probabilities = prediction_task . get ( timeout = 120 ) logger . info ( f \"Prediction successful: { prediction } with probabilities: { probabilities } \" ) if isinstance ( probabilities , np . ndarray ): probabilities = probabilities . tolist () except Exception as e : logger . error ( f \"Error during prediction task: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Error during prediction: { str ( e ) } \" ) # Fetch contextual information asynchronously using Celery context_task = fetch_contextual_information . delay ( prediction , data . sensitive_structures , data . zone ) try : analysis , piste_solution = context_task . get ( timeout = 120 ) logger . info ( f \"Context fetching successful: { analysis } , { piste_solution } \" ) except Exception as e : logger . error ( f \"Error during context fetching task: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Error during context fetching: { str ( e ) } \" ) # Perform satellite data analysis start_date = ( datetime . now () - timedelta ( days = 365 )) . strftime ( \"%Y%m %d \" ) end_date = datetime . now () . strftime ( \"%Y%m %d \" ) satellite_analysis_task = analyze_incident_zone . delay ( data . latitude , data . longitude , data . zone , prediction , start_date , end_date ) try : satellite_analysis = satellite_analysis_task . get ( timeout = 120 ) except Exception as e : logger . error ( f \"Error during satellite analysis task: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Error during satellite analysis: { str ( e ) } \" ) # Add satellite analysis to the existing analysis analysis += \" \\n\\n \" + satellite_analysis [ 'textual_analysis' ] # Prepare the response response = { \"prediction\" : prediction , \"probabilities\" : probabilities , \"analysis\" : analysis , \"piste_solution\" : piste_solution , \"satellite_data\" : { \"ndvi_ndwi_plot\" : satellite_analysis [ 'ndvi_ndwi_plot' ], \"ndvi_heatmap\" : satellite_analysis [ 'ndvi_heatmap' ], \"landcover_plot\" : satellite_analysis [ 'landcover_plot' ], } } # Validate all required fields are present if not all ([ data . incident_id , prediction , piste_solution , analysis ]): raise HTTPException ( status_code = 400 , detail = \"Missing required fields for database insertion.\" ) # Convert prediction list to a string format for database insertion prediction_texts = [ pred [ 0 ] for pred in prediction ] prediction_texts = [ text . replace ( \"Pollution de leau\" , \"Pollution de l'eau\" ) . replace ( \"Pollution de lair\" , \"Pollution de l'air\" ) for text in prediction_texts ] # Replace the prediction text with the correct prediction text specifically for this case prediction_str = \", \" . join ( prediction_texts ) . encode ( 'utf-8' ) . decode ( 'utf-8' ) # Insert the prediction and context into the database query = \"\"\" INSERT INTO \"Mapapi_prediction\" (incident_id, incident_type, piste_solution, analysis) VALUES (:incident_id, :incident_type, :piste_solution, :analysis); \"\"\" values = { \"incident_id\" : data . incident_id , \"incident_type\" : prediction_str , \"piste_solution\" : piste_solution , \"analysis\" : analysis , } try : await database . execute ( query = query , values = values ) logger . info ( \"Database insertion successful\" ) except Exception as e : logger . error ( f \"Database error: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Database error: { str ( e ) } \" ) return JSONResponse ( content = response ) except HTTPException as http_exc : # Re-raise HTTPExceptions to be handled by the global exception handler raise http_exc except Exception as e : logger . error ( f \"Unhandled exception: { e } \" ) raise HTTPException ( status_code = 500 , detail = str ( e )) sanitize_error_message ( message , sensitive_structures ) Sanitizes the error message by masking sensitive structures. Parameters: Name Type Description Default message str The original error message. required sensitive_structures List [ str ] A list of sensitive terms to mask. required Returns: Name Type Description str str The sanitized error message. Source code in app/apis/main_router.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def sanitize_error_message ( message : str , sensitive_structures : List [ str ]) -> str : \"\"\" Sanitizes the error message by masking sensitive structures. Args: message (str): The original error message. sensitive_structures (List[str]): A list of sensitive terms to mask. Returns: str: The sanitized error message. \"\"\" sanitized_message = message for structure in sensitive_structures : sanitized_message = sanitized_message . replace ( structure , \"***\" ) return sanitized_message save_chat_history ( chat_key , question , answer ) async Saves a single chat interaction to the database. Parameters: Name Type Description Default chat_key str The unique identifier for the chat session. required question str The user's question. required answer str The assistant's answer. required Raises: Type Description HTTPException If there is an error saving to the database. Source code in app/apis/main_router.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 async def save_chat_history ( chat_key : str , question : str , answer : str ): \"\"\" Saves a single chat interaction to the database. Args: chat_key (str): The unique identifier for the chat session. question (str): The user's question. answer (str): The assistant's answer. Raises: HTTPException: If there is an error saving to the database. \"\"\" query = \"\"\" INSERT INTO \"Mapapi_chathistory\" (session_id, question, answer) VALUES (:session_id, :question, :answer); \"\"\" values = { \"session_id\" : chat_key , \"question\" : question , \"answer\" : answer , } try : await database . execute ( query = query , values = values ) logger . info ( f \"Chat history saved for session { chat_key } \" ) except Exception as e : logger . error ( f \"Error saving chat history: { e } \" )","title":"Api route"},{"location":"api_route/#apis.main_router.chat_endpoint","text":"WebSocket endpoint for handling chat interactions and chat history deletion. Parameters: Name Type Description Default websocket WebSocket The WebSocket connection. required Raises: Type Description WebSocketDisconnect If the connection is closed. Source code in app/apis/main_router.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 @router . websocket ( \"/ws/chat\" ) async def chat_endpoint ( websocket : WebSocket ): \"\"\" WebSocket endpoint for handling chat interactions and chat history deletion. Args: websocket (WebSocket): The WebSocket connection. Raises: WebSocketDisconnect: If the connection is closed. \"\"\" logger . info ( f \"WebSocket connection attempt from { websocket . client . host } \" ) origin = websocket . headers . get ( \"origin\" ) logger . info ( f \"Received Origin: { origin } \" ) allowed_origins = [ \"http://197.155.176.134\" , \"http://localhost:3000\" , \"http://127.0.0.1\" , \"http://57.153.185.160\" , \"https://app.map-action.com\" , \"http://app.map-action.com\" , None , ] if origin not in allowed_origins : logger . warning ( f \"Connection rejected from origin: { origin } \" ) await websocket . close ( code = status . WS_1008_POLICY_VIOLATION ) return logger . info ( f \"Connection accepted from origin: { origin } \" ) await manager . connect ( websocket ) try : while True : data = await websocket . receive_json () action = data . get ( \"action\" ) incident_id = data . get ( \"incident_id\" ) session_id = data . get ( \"session_id\" ) if not incident_id or not session_id : logger . error ( \"Missing incident_id or session_id in message\" ) await websocket . send_json ({ \"error\" : \"Missing incident_id or session_id\" }) continue if action == \"delete_chat\" : # Handle chat history deletion chat_key = f \" { session_id }{ incident_id } \" query = \"\"\" DELETE FROM \"Mapapi_chathistory\" WHERE session_id = :session_id; \"\"\" values = { \"session_id\" : chat_key } try : await database . execute ( query = query , values = values ) logger . info ( f \"Chat history deleted for session { chat_key } \" ) # Clear in-memory chat history if chat_key in chat_histories : del chat_histories [ chat_key ] # Send a confirmation to the client await websocket . send_json ({ \"message\" : \"Chat history deleted successfully.\" }) except Exception as e : logger . error ( f \"Error deleting chat history: { e } \" ) await websocket . send_json ({ \"error\" : \"Error deleting chat history.\" }) else : # Fetch context from the database based on incident_id query = \"\"\" SELECT incident_type, analysis, piste_solution FROM \"Mapapi_prediction\" WHERE incident_id = :incident_id; \"\"\" values = { \"incident_id\" : incident_id } result = await database . fetch_one ( query = query , values = values ) if result : context_obj = { \"type_incident\" : result [ \"incident_type\" ], \"analysis\" : result [ \"analysis\" ], \"piste_solution\" : result [ \"piste_solution\" ], } context = json . dumps ( context_obj ) # Retrieve impact_area from in-memory storage impact_area = impact_area_storage . get ( incident_id , None ) else : logger . error ( f \"No context found for incident_id: { incident_id } \" ) await websocket . send_json ({ \"error\" : \"No context found for the given incident_id\" }) await websocket . close ( code = status . WS_1008_POLICY_VIOLATION ) return chat_key = f \" { session_id }{ incident_id } \" # Initialize chat history if not present if chat_key not in chat_histories : history_query = \"\"\" SELECT question, answer FROM \"Mapapi_chathistory\" WHERE session_id = :session_id ORDER BY id ASC; \"\"\" history_values = { \"session_id\" : chat_key } history_results = await database . fetch_all ( query = history_query , values = history_values ) chat_histories [ chat_key ] = [ { \"role\" : \"user\" , \"content\" : record [ \"question\" ]} for record in history_results ] + [ { \"role\" : \"assistant\" , \"content\" : record [ \"answer\" ]} for record in history_results ] # Add the user's question to the chat history question = data . get ( \"question\" ) chat_histories [ chat_key ] . append ({ \"role\" : \"user\" , \"content\" : question }) # Get response from chat bot chatbot_response = chat_response ( question , context , chat_histories [ chat_key ], impact_area ) # Append assistant's response to history chat_histories [ chat_key ] . append ({ \"role\" : \"assistant\" , \"content\" : chatbot_response }) # Send the response back through the WebSocket response_message = { \"incident_id\" : incident_id , \"session_id\" : session_id , \"question\" : question , \"answer\" : chatbot_response , } await websocket . send_json ( response_message ) # Save the chat history to the database await save_chat_history ( chat_key , question , chatbot_response ) except WebSocketDisconnect : manager . disconnect ( websocket ) logger . info ( f \"WebSocket disconnected from { websocket . client . host } \" ) except Exception as e : logger . error ( f \"WebSocket error: { e } \" ) await websocket . close ( code = status . WS_1011_INTERNAL_ERROR )","title":"chat_endpoint"},{"location":"api_route/#apis.main_router.construct_image_url","text":"Constructs the full URL for the image based on the image name. Parameters: Name Type Description Default image_name str The name or path of the image. required Returns: Name Type Description str str The full URL to access the image. Source code in app/apis/main_router.py 41 42 43 44 45 46 47 48 49 50 51 def construct_image_url ( image_name : str ) -> str : \"\"\" Constructs the full URL for the image based on the image name. Args: image_name (str): The name or path of the image. Returns: str: The full URL to access the image. \"\"\" return f \" { BASE_URL } / { image_name . split ( '/' )[ - 1 ] } \"","title":"construct_image_url"},{"location":"api_route/#apis.main_router.fetch_image","text":"Fetches the image from the specified URL. Parameters: Name Type Description Default image_url str The URL of the image to fetch. required Returns: Name Type Description bytes bytes The binary content of the fetched image. Raises: Type Description HTTPException If the image cannot be fetched. Source code in app/apis/main_router.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 async def fetch_image ( image_url : str ) -> bytes : \"\"\" Fetches the image from the specified URL. Args: image_url (str): The URL of the image to fetch. Returns: bytes: The binary content of the fetched image. Raises: HTTPException: If the image cannot be fetched. \"\"\" try : response = requests . get ( image_url ) response . raise_for_status () return response . content except requests . RequestException as e : logger . error ( f \"Failed to fetch image from { image_url } : { str ( e ) } \" ) raise HTTPException ( status_code = 500 , detail = f \"Failed to fetch image: { str ( e ) } \" )","title":"fetch_image"},{"location":"api_route/#apis.main_router.get_chat_history","text":"Retrieves the chat history for a given chat_key. Parameters: Name Type Description Default chat_key str The unique identifier for the chat session. required Returns: Name Type Description list A list of chat messages in chronological order. Raises: Type Description HTTPException If there is an error fetching the chat history. Source code in app/apis/main_router.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 @router . get ( \"/MapApi/history/ {chat_key} \" ) async def get_chat_history ( chat_key : str ): \"\"\" Retrieves the chat history for a given chat_key. Args: chat_key (str): The unique identifier for the chat session. Returns: list: A list of chat messages in chronological order. Raises: HTTPException: If there is an error fetching the chat history. \"\"\" query = \"\"\" SELECT question, answer FROM \"Mapapi_chathistory\" WHERE session_id = :session_id ORDER BY id ASC; \"\"\" values = { \"session_id\" : chat_key } try : results = await database . fetch_all ( query = query , values = values ) # Format the results to interleave user and assistant messages formatted_history = [] for record in results : formatted_history . append ({ \"role\" : \"user\" , \"content\" : record [ \"question\" ]}) formatted_history . append ({ \"role\" : \"assistant\" , \"content\" : record [ \"answer\" ]}) return formatted_history except Exception as e : logger . error ( f \"Error fetching chat history: { e } \" ) raise HTTPException ( status_code = 500 , detail = \"Error fetching chat history\" )","title":"get_chat_history"},{"location":"api_route/#apis.main_router.index","text":"Root endpoint to verify that the API is running. Returns: Name Type Description dict A message indicating the API is operational. Source code in app/apis/main_router.py 90 91 92 93 94 95 96 97 98 @router . get ( \"/\" ) def index (): \"\"\" Root endpoint to verify that the API is running. Returns: dict: A message indicating the API is operational. \"\"\" return { \"message\" : \"Map Action classification model\" }","title":"index"},{"location":"api_route/#apis.main_router.predict_incident_type","text":"Predicts the type of incident based on the provided image and other data. Parameters: Name Type Description Default data ImageModel The input data containing image name, sensitive structures, zone, and incident ID. required Returns: Name Type Description JSONResponse The prediction results including incident type, probabilities, context, impact, and solution. Raises: Type Description HTTPException If any step in the prediction process fails. Source code in app/apis/main_router.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @router . post ( \"/image/predict\" ) async def predict_incident_type ( data : ImageModel ): \"\"\" Predicts the type of incident based on the provided image and other data. Args: data (ImageModel): The input data containing image name, sensitive structures, zone, and incident ID. Returns: JSONResponse: The prediction results including incident type, probabilities, context, impact, and solution. Raises: HTTPException: If any step in the prediction process fails. \"\"\" try : logger . info ( f \"Received request for image: { data . image_name } with sensitive structures: { data . sensitive_structures } , incident_id: { data . incident_id } in zone: { data . zone } \" ) image_url = construct_image_url ( data . image_name ) image = await fetch_image ( image_url ) # Perform prediction asynchronously using Celery prediction_task = perform_prediction . delay ( image ) try : prediction , probabilities = prediction_task . get ( timeout = 120 ) logger . info ( f \"Prediction successful: { prediction } with probabilities: { probabilities } \" ) if isinstance ( probabilities , np . ndarray ): probabilities = probabilities . tolist () except Exception as e : logger . error ( f \"Error during prediction task: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Error during prediction: { str ( e ) } \" ) # Fetch contextual information asynchronously using Celery context_task = fetch_contextual_information . delay ( prediction , data . sensitive_structures , data . zone ) try : analysis , piste_solution = context_task . get ( timeout = 120 ) logger . info ( f \"Context fetching successful: { analysis } , { piste_solution } \" ) except Exception as e : logger . error ( f \"Error during context fetching task: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Error during context fetching: { str ( e ) } \" ) # Perform satellite data analysis start_date = ( datetime . now () - timedelta ( days = 365 )) . strftime ( \"%Y%m %d \" ) end_date = datetime . now () . strftime ( \"%Y%m %d \" ) satellite_analysis_task = analyze_incident_zone . delay ( data . latitude , data . longitude , data . zone , prediction , start_date , end_date ) try : satellite_analysis = satellite_analysis_task . get ( timeout = 120 ) except Exception as e : logger . error ( f \"Error during satellite analysis task: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Error during satellite analysis: { str ( e ) } \" ) # Add satellite analysis to the existing analysis analysis += \" \\n\\n \" + satellite_analysis [ 'textual_analysis' ] # Prepare the response response = { \"prediction\" : prediction , \"probabilities\" : probabilities , \"analysis\" : analysis , \"piste_solution\" : piste_solution , \"satellite_data\" : { \"ndvi_ndwi_plot\" : satellite_analysis [ 'ndvi_ndwi_plot' ], \"ndvi_heatmap\" : satellite_analysis [ 'ndvi_heatmap' ], \"landcover_plot\" : satellite_analysis [ 'landcover_plot' ], } } # Validate all required fields are present if not all ([ data . incident_id , prediction , piste_solution , analysis ]): raise HTTPException ( status_code = 400 , detail = \"Missing required fields for database insertion.\" ) # Convert prediction list to a string format for database insertion prediction_texts = [ pred [ 0 ] for pred in prediction ] prediction_texts = [ text . replace ( \"Pollution de leau\" , \"Pollution de l'eau\" ) . replace ( \"Pollution de lair\" , \"Pollution de l'air\" ) for text in prediction_texts ] # Replace the prediction text with the correct prediction text specifically for this case prediction_str = \", \" . join ( prediction_texts ) . encode ( 'utf-8' ) . decode ( 'utf-8' ) # Insert the prediction and context into the database query = \"\"\" INSERT INTO \"Mapapi_prediction\" (incident_id, incident_type, piste_solution, analysis) VALUES (:incident_id, :incident_type, :piste_solution, :analysis); \"\"\" values = { \"incident_id\" : data . incident_id , \"incident_type\" : prediction_str , \"piste_solution\" : piste_solution , \"analysis\" : analysis , } try : await database . execute ( query = query , values = values ) logger . info ( \"Database insertion successful\" ) except Exception as e : logger . error ( f \"Database error: { e } \" ) raise HTTPException ( status_code = 500 , detail = f \"Database error: { str ( e ) } \" ) return JSONResponse ( content = response ) except HTTPException as http_exc : # Re-raise HTTPExceptions to be handled by the global exception handler raise http_exc except Exception as e : logger . error ( f \"Unhandled exception: { e } \" ) raise HTTPException ( status_code = 500 , detail = str ( e ))","title":"predict_incident_type"},{"location":"api_route/#apis.main_router.sanitize_error_message","text":"Sanitizes the error message by masking sensitive structures. Parameters: Name Type Description Default message str The original error message. required sensitive_structures List [ str ] A list of sensitive terms to mask. required Returns: Name Type Description str str The sanitized error message. Source code in app/apis/main_router.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def sanitize_error_message ( message : str , sensitive_structures : List [ str ]) -> str : \"\"\" Sanitizes the error message by masking sensitive structures. Args: message (str): The original error message. sensitive_structures (List[str]): A list of sensitive terms to mask. Returns: str: The sanitized error message. \"\"\" sanitized_message = message for structure in sensitive_structures : sanitized_message = sanitized_message . replace ( structure , \"***\" ) return sanitized_message","title":"sanitize_error_message"},{"location":"api_route/#apis.main_router.save_chat_history","text":"Saves a single chat interaction to the database. Parameters: Name Type Description Default chat_key str The unique identifier for the chat session. required question str The user's question. required answer str The assistant's answer. required Raises: Type Description HTTPException If there is an error saving to the database. Source code in app/apis/main_router.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 async def save_chat_history ( chat_key : str , question : str , answer : str ): \"\"\" Saves a single chat interaction to the database. Args: chat_key (str): The unique identifier for the chat session. question (str): The user's question. answer (str): The assistant's answer. Raises: HTTPException: If there is an error saving to the database. \"\"\" query = \"\"\" INSERT INTO \"Mapapi_chathistory\" (session_id, question, answer) VALUES (:session_id, :question, :answer); \"\"\" values = { \"session_id\" : chat_key , \"question\" : question , \"answer\" : answer , } try : await database . execute ( query = query , values = values ) logger . info ( f \"Chat history saved for session { chat_key } \" ) except Exception as e : logger . error ( f \"Error saving chat history: { e } \" )","title":"save_chat_history"},{"location":"celery/","text":"make_celery () Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Name Type Description Celery A Celery application instance configured to use Redis for queuing tasks and storing results. Source code in app/services/celery/celery_config.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def make_celery (): \"\"\" Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Celery: A Celery application instance configured to use Redis for queuing tasks and storing results. \"\"\" # Retrieve Redis connection details from environment variables redis_host = os . getenv ( 'REDIS_HOST' , 'redis' ) redis_port = os . getenv ( 'REDIS_PORT' , 6379 ) redis_username = os . getenv ( 'REDIS_USERNAME' , '' ) redis_password = os . getenv ( 'REDIS_PASSWORD' , '' ) redis_url = os . getenv ( 'REDIS_URL' ) # Fetch the Redis URL from the environment variable celery = Celery ( 'worker' , # Name of the worker backend = redis_url , # Use Redis URL from environment broker = redis_url # Use Redis URL from environment ) return celery analyze_incident_zone ( lat , lon , incident_location , incident_type , start_date , end_date ) Analyze the incident zone using satellite data. Returns: dict: A dictionary containing analysis results and plot data. Source code in app/services/celery/celery_task.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 @celery_app . task def analyze_incident_zone ( lat , lon , incident_location , incident_type , start_date , end_date ) -> dict : \"\"\" Analyze the incident zone using satellite data. Returns: dict: A dictionary containing analysis results and plot data. \"\"\" logging . info ( f \"Analyzing incident zone for { incident_type } at { incident_location } \" ) # Create Earth Engine point and buffered area point = ee . Geometry . Point ([ lon , lat ]) buffered_point = point . buffer ( 500 ) # 500-meter buffer # Convert dates to datetime objects start_date = datetime . strptime ( start_date , '%Y%m %d ' ) end_date = datetime . strptime ( end_date , '%Y%m %d ' ) # Perform satellite data analysis ndvi_data , ndwi_data = analyze_vegetation_and_water ( point , buffered_point , start_date , end_date ) landcover_data = analyze_land_cover ( buffered_point ) # Generate plots ndvi_ndwi_plot = generate_ndvi_ndwi_plot ( ndvi_data , ndwi_data ) ndvi_heatmap = generate_ndvi_heatmap ( ndvi_data ) landcover_plot = generate_landcover_plot ( landcover_data ) # Generate textual analysis using the new LLM function textual_analysis = generate_satellite_analysis ( ndvi_data , ndwi_data , landcover_data , incident_type ) # Prepare return dictionary result = { 'textual_analysis' : textual_analysis , 'ndvi_ndwi_plot' : ndvi_ndwi_plot , 'ndvi_heatmap' : ndvi_heatmap , 'landcover_plot' : landcover_plot , 'raw_data' : { 'ndvi' : ndvi_data . to_dict (), 'ndwi' : ndwi_data . to_dict (), 'landcover' : landcover_data } } return result fetch_contextual_information ( prediction , sensitive_structures , zone ) A Celery task that fetches contextual information based on the prediction, sensitive structures, and additional data. Parameters: Name Type Description Default prediction str The predicted classification. required sensitive_structures list A list of sensitive structures related to the prediction. required zone str The geographic zone related to the prediction. required Returns: Name Type Description tuple A tuple containing analysis and piste_solution, both formatted in markdown. Source code in app/services/celery/celery_task.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 @celery_app . task def fetch_contextual_information ( prediction , sensitive_structures , zone ): \"\"\" A Celery task that fetches contextual information based on the prediction, sensitive structures, and additional data. Args: prediction (str): The predicted classification. sensitive_structures (list): A list of sensitive structures related to the prediction. zone (str): The geographic zone related to the prediction. Returns: tuple: A tuple containing analysis and piste_solution, both formatted in markdown. \"\"\" try : logger . info ( \"Starting contextual information task.\" ) system_message = f \"\"\" <system> <role>assistant AI</role> <task>analyse des incidents environnementaux avec formatage markdown</task> <location>Mali</location> <incident> <type> { prediction } </type> <zone> { zone } </zone> <sensitive_structures> { ', ' . join ( sensitive_structures ) } </sensitive_structures> </incident> <instructions> <instruction>Analysez la nature du probl\u00e8me et ses cons\u00e9quences imm\u00e9diates dans la zone sp\u00e9cifi\u00e9e.</instruction> <instruction>Identifiez les risques sp\u00e9cifiques aux infrastructures locales (routes, h\u00f4pitaux, \u00e9coles) dans la zone indiqu\u00e9e.</instruction> <instruction>\u00c9valuez les cons\u00e9quences environnementales dans la zone, telles que la pollution, la contamination des eaux, et la perte de biodiversit\u00e9.</instruction> <instruction>D\u00e9terminez les acteurs locaux \u00e0 mobiliser dans la zone pour r\u00e9soudre ce probl\u00e8me.</instruction> <instruction>\u00c9valuez les impacts \u00e9conomiques et sociaux \u00e0 long terme en tenant compte des caract\u00e9ristiques sp\u00e9cifiques de la zone.</instruction> <instruction>Formatez la r\u00e9ponse en utilisant la syntaxe markdown appropri\u00e9e.</instruction> </instructions> <response_formatting> <formatting_rule>Utilisez '**' pour les titres principaux ex: **Titre**.</formatting_rule> <formatting_rule>Utilisez '***texte***' pour mettre en gras et en italique les chiffres, pourcentages ex: ***100***.</formatting_rule> <formatting_rule>Utilisez '- ' au d\u00e9but d'une ligne pour les listes \u00e0 puces.</formatting_rule> <formatting_rule>Laissez une ligne vide entre chaque paragraphe pour bien espacer le contenu.</formatting_rule> <formatting_rule>Structurez la r\u00e9ponse en sections claires avec des titres appropri\u00e9s.</formatting_rule> <formatting_rule>Commencez par le probl\u00e8me principal dans la zone sp\u00e9cifi\u00e9e, puis \u00e9noncez les solutions propos\u00e9es ou les impacts analys\u00e9s.</formatting_rule> <formatting_rule>Utilisez des mots simples et clairs, \u00e9vitez le jargon technique inutile.</formatting_rule> <formatting_rule>Donnez des informations essentielles en utilisant un langage direct et pr\u00e9cis.</formatting_rule> <formatting_rule>Si une recommandation est faite, assurez-vous qu'elle est faisable et contextualis\u00e9e pour la zone en question.</formatting_rule> </response_formatting> <examples> <example> <prompt>Analysez l'impact de la pollution de l'eau sur les infrastructures locales dans la r\u00e9gion de Bamako.</prompt> <response> ** Impact de la pollution de l'eau \u00e0 Bamako ** La pollution de l'eau dans la r\u00e9gion de Bamako affecte directement les infrastructures locales, notamment les syst\u00e8mes d'approvisionnement en eau potable. Les cons\u00e9quences principales sont : - ***Contamination des sources d'eau*** par des rejets industriels non contr\u00f4l\u00e9s - ***Co\u00fbts suppl\u00e9mentaires*** pour le traitement de l'eau - ***Risques sanitaires*** accrus pour les habitants ** Impacts sur les infrastructures sensibles ** - ***\u00c9coles et h\u00f4pitaux*** : D\u00e9pendance \u00e0 l'eau contamin\u00e9e, n\u00e9cessitant une intervention rapide - ***Syst\u00e8mes de distribution*** : D\u00e9t\u00e9rioration acc\u00e9l\u00e9r\u00e9e due aux polluants Une action imm\u00e9diate est n\u00e9cessaire pour \u00e9viter des cons\u00e9quences sanitaires graves et des co\u00fbts \u00e0 long terme pour la municipalit\u00e9. </response> </example> </examples> </system> \"\"\" solution_prompt = f \"\"\" <system> <role>assistant AI</role> <task>recommandations de solutions pour des incidents environnementaux avec formatage markdown</task> <incident> <type> { prediction } </type> <zone> { zone } </zone> <sensitive_structures> { ', ' . join ( sensitive_structures ) } </sensitive_structures> </incident> <instructions> <instruction>Recommandez des solutions sp\u00e9cifiques en tenant compte du type de terrain, des infrastructures \u00e0 proximit\u00e9, et des \u00e9cosyst\u00e8mes sensibles dans la zone sp\u00e9cifi\u00e9e.</instruction> <instruction>Proposez des mesures pr\u00e9ventives et curatives adapt\u00e9es \u00e0 la zone pour \u00e9viter que le probl\u00e8me ne se reproduise.</instruction> <instruction>Sugg\u00e9rez des collaborations entre les autorit\u00e9s locales, les ONG, et les entreprises pour mettre en \u0153uvre les solutions dans la zone concern\u00e9e.</instruction> <instruction>Formatez la r\u00e9ponse en utilisant la syntaxe markdown appropri\u00e9e.</instruction> </instructions> <response_formatting> <formatting_rule>Utilisez '**' suivi d'un espace pour les titres principaux.</formatting_rule> <formatting_rule>Utilisez '***texte***' pour mettre en gras et en italique les chiffres, pourcentages et termes cl\u00e9s.</formatting_rule> <formatting_rule>Utilisez '- ' au d\u00e9but d'une ligne pour les listes \u00e0 puces.</formatting_rule> <formatting_rule>Laissez une ligne vide entre chaque paragraphe pour bien espacer le contenu.</formatting_rule> <formatting_rule>Structurez la r\u00e9ponse en sections claires avec des titres appropri\u00e9s.</formatting_rule> <formatting_rule>Commencez par la solution la plus imm\u00e9diate et pertinente pour la zone sp\u00e9cifi\u00e9e.</formatting_rule> <formatting_rule>Utilisez des mots simples et clairs, \u00e9vitez le jargon technique inutile.</formatting_rule> </response_formatting> <examples> <example> <prompt>Quelles sont les mesures pr\u00e9ventives \u00e0 mettre en place pour \u00e9viter la d\u00e9forestation dans la zone de Sikasso ?</prompt> <response> ** Mesures pr\u00e9ventives contre la d\u00e9forestation \u00e0 Sikasso ** ** Renforcement des politiques foresti\u00e8res ** - Mise en place d'une ***r\u00e9gulation stricte des coupes d'arbres*** - D\u00e9veloppement de ***programmes de gestion durable des for\u00eats*** ** Sensibilisation et \u00e9ducation ** - ***Campagnes d'information*** sur l'importance des for\u00eats pour les communaut\u00e9s locales - Promotion de l'***agroforesterie*** comme alternative durable ** Partenariats et reboisement ** - Collaboration avec des ***ONG sp\u00e9cialis\u00e9es*** pour des projets de reboisement - Cr\u00e9ation de ***p\u00e9pini\u00e8res communautaires*** pour la production de jeunes arbres Ces mesures, adapt\u00e9es au contexte local de Sikasso, visent \u00e0 pr\u00e9server les for\u00eats existantes tout en encourageant des pratiques durables pour l'avenir. </response> </example> </examples> </system> \"\"\" # Fetching responses from the model analysis = get_response ( system_message ) piste_solution = get_response ( solution_prompt ) logger . info ( f \"Analysis: { analysis } , Solution: { piste_solution } \" ) return analysis , piste_solution except Exception as e : logger . error ( f \"Contextual information task failed: { str ( e ) } \" ) return { \"error\" : str ( e )}, None initialize_earth_engine () Initialize Earth Engine with service account credentials. Source code in app/services/celery/celery_task.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def initialize_earth_engine (): \"\"\" Initialize Earth Engine with service account credentials. \"\"\" try : credentials = ee . ServiceAccountCredentials ( email = os . environ [ 'GEE_SERVICE_ACCOUNT_EMAIL' ], key_file = os . environ [ 'GEE_SERVICE_ACCOUNT_KEY_FILE' ] ) ee . Initialize ( credentials ) logging . info ( \"Earth Engine initialized successfully.\" ) except Exception as e : logging . error ( f \"Failed to initialize Earth Engine: { str ( e ) } \" ) raise perform_prediction ( image ) A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Parameters: Name Type Description Default image bytes The image data in bytes format, ready to be processed by the prediction model. required Returns: Name Type Description tuple A tuple containing the predicted classification and a list of probabilities associated with each class. Source code in app/services/celery/celery_task.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 @celery_app . task def perform_prediction ( image ): \"\"\" A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Args: image (bytes): The image data in bytes format, ready to be processed by the prediction model. Returns: tuple: A tuple containing the predicted classification and a list of probabilities associated with each class. \"\"\" try : logger . info ( \"Starting prediction task.\" ) prediction , probabilities = predict ( image ) logger . info ( f \"Prediction: { prediction } , Probabilities: { probabilities } \" ) # Ensure `probabilities` is a list if isinstance ( probabilities , list ): return prediction , probabilities else : return prediction , probabilities . tolist () except Exception as e : logger . error ( f \"Prediction task failed: { str ( e ) } \" ) return { \"error\" : str ( e )}, [] run_prediction_and_context ( image , sensitive_structures ) Chain the prediction task with the contextual information task. Source code in app/services/celery/celery_task.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 @celery_app . task def run_prediction_and_context ( image , sensitive_structures ): \"\"\" Chain the prediction task with the contextual information task. \"\"\" try : logger . info ( \"Starting chained task: run_prediction_and_context.\" ) prediction , probabilities = perform_prediction ( image ) # Proceed with fetching contextual information only if prediction is successful if prediction and not isinstance ( prediction , dict ): # Ensure it's not an error dict context_info = fetch_contextual_information . delay ( prediction , sensitive_structures ) return context_info . get ( timeout = 120 ) else : logger . error ( f \"Failed to proceed due to prediction error: { prediction } \" ) return { \"error\" : \"Prediction failed, unable to fetch contextual information.\" } except Exception as e : logger . error ( f \"Chained task failed: { e } \" ) return { \"error\" : str ( e )}","title":"celery config and tasks"},{"location":"celery/#services.celery.celery_config.make_celery","text":"Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Name Type Description Celery A Celery application instance configured to use Redis for queuing tasks and storing results. Source code in app/services/celery/celery_config.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def make_celery (): \"\"\" Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Celery: A Celery application instance configured to use Redis for queuing tasks and storing results. \"\"\" # Retrieve Redis connection details from environment variables redis_host = os . getenv ( 'REDIS_HOST' , 'redis' ) redis_port = os . getenv ( 'REDIS_PORT' , 6379 ) redis_username = os . getenv ( 'REDIS_USERNAME' , '' ) redis_password = os . getenv ( 'REDIS_PASSWORD' , '' ) redis_url = os . getenv ( 'REDIS_URL' ) # Fetch the Redis URL from the environment variable celery = Celery ( 'worker' , # Name of the worker backend = redis_url , # Use Redis URL from environment broker = redis_url # Use Redis URL from environment ) return celery","title":"make_celery"},{"location":"celery/#services.celery.celery_task.analyze_incident_zone","text":"Analyze the incident zone using satellite data. Returns: dict: A dictionary containing analysis results and plot data. Source code in app/services/celery/celery_task.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 @celery_app . task def analyze_incident_zone ( lat , lon , incident_location , incident_type , start_date , end_date ) -> dict : \"\"\" Analyze the incident zone using satellite data. Returns: dict: A dictionary containing analysis results and plot data. \"\"\" logging . info ( f \"Analyzing incident zone for { incident_type } at { incident_location } \" ) # Create Earth Engine point and buffered area point = ee . Geometry . Point ([ lon , lat ]) buffered_point = point . buffer ( 500 ) # 500-meter buffer # Convert dates to datetime objects start_date = datetime . strptime ( start_date , '%Y%m %d ' ) end_date = datetime . strptime ( end_date , '%Y%m %d ' ) # Perform satellite data analysis ndvi_data , ndwi_data = analyze_vegetation_and_water ( point , buffered_point , start_date , end_date ) landcover_data = analyze_land_cover ( buffered_point ) # Generate plots ndvi_ndwi_plot = generate_ndvi_ndwi_plot ( ndvi_data , ndwi_data ) ndvi_heatmap = generate_ndvi_heatmap ( ndvi_data ) landcover_plot = generate_landcover_plot ( landcover_data ) # Generate textual analysis using the new LLM function textual_analysis = generate_satellite_analysis ( ndvi_data , ndwi_data , landcover_data , incident_type ) # Prepare return dictionary result = { 'textual_analysis' : textual_analysis , 'ndvi_ndwi_plot' : ndvi_ndwi_plot , 'ndvi_heatmap' : ndvi_heatmap , 'landcover_plot' : landcover_plot , 'raw_data' : { 'ndvi' : ndvi_data . to_dict (), 'ndwi' : ndwi_data . to_dict (), 'landcover' : landcover_data } } return result","title":"analyze_incident_zone"},{"location":"celery/#services.celery.celery_task.fetch_contextual_information","text":"A Celery task that fetches contextual information based on the prediction, sensitive structures, and additional data. Parameters: Name Type Description Default prediction str The predicted classification. required sensitive_structures list A list of sensitive structures related to the prediction. required zone str The geographic zone related to the prediction. required Returns: Name Type Description tuple A tuple containing analysis and piste_solution, both formatted in markdown. Source code in app/services/celery/celery_task.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 @celery_app . task def fetch_contextual_information ( prediction , sensitive_structures , zone ): \"\"\" A Celery task that fetches contextual information based on the prediction, sensitive structures, and additional data. Args: prediction (str): The predicted classification. sensitive_structures (list): A list of sensitive structures related to the prediction. zone (str): The geographic zone related to the prediction. Returns: tuple: A tuple containing analysis and piste_solution, both formatted in markdown. \"\"\" try : logger . info ( \"Starting contextual information task.\" ) system_message = f \"\"\" <system> <role>assistant AI</role> <task>analyse des incidents environnementaux avec formatage markdown</task> <location>Mali</location> <incident> <type> { prediction } </type> <zone> { zone } </zone> <sensitive_structures> { ', ' . join ( sensitive_structures ) } </sensitive_structures> </incident> <instructions> <instruction>Analysez la nature du probl\u00e8me et ses cons\u00e9quences imm\u00e9diates dans la zone sp\u00e9cifi\u00e9e.</instruction> <instruction>Identifiez les risques sp\u00e9cifiques aux infrastructures locales (routes, h\u00f4pitaux, \u00e9coles) dans la zone indiqu\u00e9e.</instruction> <instruction>\u00c9valuez les cons\u00e9quences environnementales dans la zone, telles que la pollution, la contamination des eaux, et la perte de biodiversit\u00e9.</instruction> <instruction>D\u00e9terminez les acteurs locaux \u00e0 mobiliser dans la zone pour r\u00e9soudre ce probl\u00e8me.</instruction> <instruction>\u00c9valuez les impacts \u00e9conomiques et sociaux \u00e0 long terme en tenant compte des caract\u00e9ristiques sp\u00e9cifiques de la zone.</instruction> <instruction>Formatez la r\u00e9ponse en utilisant la syntaxe markdown appropri\u00e9e.</instruction> </instructions> <response_formatting> <formatting_rule>Utilisez '**' pour les titres principaux ex: **Titre**.</formatting_rule> <formatting_rule>Utilisez '***texte***' pour mettre en gras et en italique les chiffres, pourcentages ex: ***100***.</formatting_rule> <formatting_rule>Utilisez '- ' au d\u00e9but d'une ligne pour les listes \u00e0 puces.</formatting_rule> <formatting_rule>Laissez une ligne vide entre chaque paragraphe pour bien espacer le contenu.</formatting_rule> <formatting_rule>Structurez la r\u00e9ponse en sections claires avec des titres appropri\u00e9s.</formatting_rule> <formatting_rule>Commencez par le probl\u00e8me principal dans la zone sp\u00e9cifi\u00e9e, puis \u00e9noncez les solutions propos\u00e9es ou les impacts analys\u00e9s.</formatting_rule> <formatting_rule>Utilisez des mots simples et clairs, \u00e9vitez le jargon technique inutile.</formatting_rule> <formatting_rule>Donnez des informations essentielles en utilisant un langage direct et pr\u00e9cis.</formatting_rule> <formatting_rule>Si une recommandation est faite, assurez-vous qu'elle est faisable et contextualis\u00e9e pour la zone en question.</formatting_rule> </response_formatting> <examples> <example> <prompt>Analysez l'impact de la pollution de l'eau sur les infrastructures locales dans la r\u00e9gion de Bamako.</prompt> <response> ** Impact de la pollution de l'eau \u00e0 Bamako ** La pollution de l'eau dans la r\u00e9gion de Bamako affecte directement les infrastructures locales, notamment les syst\u00e8mes d'approvisionnement en eau potable. Les cons\u00e9quences principales sont : - ***Contamination des sources d'eau*** par des rejets industriels non contr\u00f4l\u00e9s - ***Co\u00fbts suppl\u00e9mentaires*** pour le traitement de l'eau - ***Risques sanitaires*** accrus pour les habitants ** Impacts sur les infrastructures sensibles ** - ***\u00c9coles et h\u00f4pitaux*** : D\u00e9pendance \u00e0 l'eau contamin\u00e9e, n\u00e9cessitant une intervention rapide - ***Syst\u00e8mes de distribution*** : D\u00e9t\u00e9rioration acc\u00e9l\u00e9r\u00e9e due aux polluants Une action imm\u00e9diate est n\u00e9cessaire pour \u00e9viter des cons\u00e9quences sanitaires graves et des co\u00fbts \u00e0 long terme pour la municipalit\u00e9. </response> </example> </examples> </system> \"\"\" solution_prompt = f \"\"\" <system> <role>assistant AI</role> <task>recommandations de solutions pour des incidents environnementaux avec formatage markdown</task> <incident> <type> { prediction } </type> <zone> { zone } </zone> <sensitive_structures> { ', ' . join ( sensitive_structures ) } </sensitive_structures> </incident> <instructions> <instruction>Recommandez des solutions sp\u00e9cifiques en tenant compte du type de terrain, des infrastructures \u00e0 proximit\u00e9, et des \u00e9cosyst\u00e8mes sensibles dans la zone sp\u00e9cifi\u00e9e.</instruction> <instruction>Proposez des mesures pr\u00e9ventives et curatives adapt\u00e9es \u00e0 la zone pour \u00e9viter que le probl\u00e8me ne se reproduise.</instruction> <instruction>Sugg\u00e9rez des collaborations entre les autorit\u00e9s locales, les ONG, et les entreprises pour mettre en \u0153uvre les solutions dans la zone concern\u00e9e.</instruction> <instruction>Formatez la r\u00e9ponse en utilisant la syntaxe markdown appropri\u00e9e.</instruction> </instructions> <response_formatting> <formatting_rule>Utilisez '**' suivi d'un espace pour les titres principaux.</formatting_rule> <formatting_rule>Utilisez '***texte***' pour mettre en gras et en italique les chiffres, pourcentages et termes cl\u00e9s.</formatting_rule> <formatting_rule>Utilisez '- ' au d\u00e9but d'une ligne pour les listes \u00e0 puces.</formatting_rule> <formatting_rule>Laissez une ligne vide entre chaque paragraphe pour bien espacer le contenu.</formatting_rule> <formatting_rule>Structurez la r\u00e9ponse en sections claires avec des titres appropri\u00e9s.</formatting_rule> <formatting_rule>Commencez par la solution la plus imm\u00e9diate et pertinente pour la zone sp\u00e9cifi\u00e9e.</formatting_rule> <formatting_rule>Utilisez des mots simples et clairs, \u00e9vitez le jargon technique inutile.</formatting_rule> </response_formatting> <examples> <example> <prompt>Quelles sont les mesures pr\u00e9ventives \u00e0 mettre en place pour \u00e9viter la d\u00e9forestation dans la zone de Sikasso ?</prompt> <response> ** Mesures pr\u00e9ventives contre la d\u00e9forestation \u00e0 Sikasso ** ** Renforcement des politiques foresti\u00e8res ** - Mise en place d'une ***r\u00e9gulation stricte des coupes d'arbres*** - D\u00e9veloppement de ***programmes de gestion durable des for\u00eats*** ** Sensibilisation et \u00e9ducation ** - ***Campagnes d'information*** sur l'importance des for\u00eats pour les communaut\u00e9s locales - Promotion de l'***agroforesterie*** comme alternative durable ** Partenariats et reboisement ** - Collaboration avec des ***ONG sp\u00e9cialis\u00e9es*** pour des projets de reboisement - Cr\u00e9ation de ***p\u00e9pini\u00e8res communautaires*** pour la production de jeunes arbres Ces mesures, adapt\u00e9es au contexte local de Sikasso, visent \u00e0 pr\u00e9server les for\u00eats existantes tout en encourageant des pratiques durables pour l'avenir. </response> </example> </examples> </system> \"\"\" # Fetching responses from the model analysis = get_response ( system_message ) piste_solution = get_response ( solution_prompt ) logger . info ( f \"Analysis: { analysis } , Solution: { piste_solution } \" ) return analysis , piste_solution except Exception as e : logger . error ( f \"Contextual information task failed: { str ( e ) } \" ) return { \"error\" : str ( e )}, None","title":"fetch_contextual_information"},{"location":"celery/#services.celery.celery_task.initialize_earth_engine","text":"Initialize Earth Engine with service account credentials. Source code in app/services/celery/celery_task.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def initialize_earth_engine (): \"\"\" Initialize Earth Engine with service account credentials. \"\"\" try : credentials = ee . ServiceAccountCredentials ( email = os . environ [ 'GEE_SERVICE_ACCOUNT_EMAIL' ], key_file = os . environ [ 'GEE_SERVICE_ACCOUNT_KEY_FILE' ] ) ee . Initialize ( credentials ) logging . info ( \"Earth Engine initialized successfully.\" ) except Exception as e : logging . error ( f \"Failed to initialize Earth Engine: { str ( e ) } \" ) raise","title":"initialize_earth_engine"},{"location":"celery/#services.celery.celery_task.perform_prediction","text":"A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Parameters: Name Type Description Default image bytes The image data in bytes format, ready to be processed by the prediction model. required Returns: Name Type Description tuple A tuple containing the predicted classification and a list of probabilities associated with each class. Source code in app/services/celery/celery_task.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 @celery_app . task def perform_prediction ( image ): \"\"\" A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Args: image (bytes): The image data in bytes format, ready to be processed by the prediction model. Returns: tuple: A tuple containing the predicted classification and a list of probabilities associated with each class. \"\"\" try : logger . info ( \"Starting prediction task.\" ) prediction , probabilities = predict ( image ) logger . info ( f \"Prediction: { prediction } , Probabilities: { probabilities } \" ) # Ensure `probabilities` is a list if isinstance ( probabilities , list ): return prediction , probabilities else : return prediction , probabilities . tolist () except Exception as e : logger . error ( f \"Prediction task failed: { str ( e ) } \" ) return { \"error\" : str ( e )}, []","title":"perform_prediction"},{"location":"celery/#services.celery.celery_task.run_prediction_and_context","text":"Chain the prediction task with the contextual information task. Source code in app/services/celery/celery_task.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 @celery_app . task def run_prediction_and_context ( image , sensitive_structures ): \"\"\" Chain the prediction task with the contextual information task. \"\"\" try : logger . info ( \"Starting chained task: run_prediction_and_context.\" ) prediction , probabilities = perform_prediction ( image ) # Proceed with fetching contextual information only if prediction is successful if prediction and not isinstance ( prediction , dict ): # Ensure it's not an error dict context_info = fetch_contextual_information . delay ( prediction , sensitive_structures ) return context_info . get ( timeout = 120 ) else : logger . error ( f \"Failed to proceed due to prediction error: { prediction } \" ) return { \"error\" : \"Prediction failed, unable to fetch contextual information.\" } except Exception as e : logger . error ( f \"Chained task failed: { e } \" ) return { \"error\" : str ( e )}","title":"run_prediction_and_context"},{"location":"cnn/","text":"","title":"cnn service"},{"location":"contribution-guideline/","text":"Map Action Contribution Guidelines Welcome to Map Action! We're excited about your interest in contributing to our open-source project. These guidelines will help you understand how to effectively contribute to our codebase. About Map Action Map Action is a Bamako-based trailblazer in using mapping technology to tackle environmental challenges and solve urban issues. Our innovative approach began by addressing Water, Sanitation, and Hygiene (WASH) problems and has grown to encompass various sectors through collaboration with civil society, governments, NGOs, and private entities. Our sustainable business model includes paid subscriptions for organizations that rely on our data to gain actionable insights across diverse domains. This model allows us to continuously enhance our offerings. Contributing We welcome contributions of all shapes and sizes! Here are the different ways you can get involved: Report bugs and request features: Identify any issues you encounter while using our tools. You can report them directly on GitHub Issues https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues. Submit code changes: If you have improvements or new features in mind, you can contribute code by creating pull requests (PRs). Write documentation or tutorials: Enhance our documentation to make it easier for others to understand and use our tools. Help with code reviews and testing: Lend your expertise by reviewing pull requests submitted by others and assisting with testing efforts. Finding Issues to Work On Browse our issue tracker on GitHub Issues https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues to find existing issues. Look for issues labeled \"help wanted\" or those categorized as bugs or enhancements that you're interested in tackling. Making Changes Fork the Repository: Visit the Map Action project on GitHub: https://github.com/223MapAction Click on the \"Fork\" button to create your own copy of the repository. Create a Branch: Clone your forked repository to your local machine. Create a new branch for your specific changes. Use a descriptive branch name that reflects your contribution (e.g., \"fix-map-loading-bug\"). Code and Commit: Make your changes to the codebase. Write clear and concise commit messages that describe your modifications. Testing: Ensure your changes don't introduce any regressions. We use pytest and flake8 for testing. Make sure your code passes all tests before submitting a pull request. Submitting Pull Requests Push to Your Branch: Once you're satisfied with your changes, push your local branch to your forked repository on GitHub. Open a Pull Request: Navigate to your forked repository on GitHub and go to the \"Pull Requests\" tab. Click on \"New pull request\" and select the branch containing your changes. Create a pull request with a clear and descriptive title and explanation of your modifications. Mention any issues your pull request addresses. Code Review: Our internal code review process involves two Map Action developers. They'll review your pull request and provide feedback or suggestions for improvement. Merge: Once your pull request is approved and any necessary changes are made, it will be merged into the main codebase. Additional Notes License: All our repositories use the GPL-3.0 license. Ensure your contributions comply with the license terms. Code of Conduct: We value a respectful and inclusive environment. Please familiarize yourself with our [Code of Conduct](CODE_OF_CONDUCT.md) before contributing. Appreciation: We appreciate all contributions, regardless of their scope. Thank you for helping us improve Map Action! We look forward to your contributions! This tailored guide incorporates the information you provided, making it specific to the Map Action project and its contribution workflow.","title":"contrubition guideline"},{"location":"install-run/","text":"Getting Started System Requirements: Python : 3.x Installation From source Clone the ML-Deploy repository: $ git clone https://github.com/223MapAction/ML-Deploy.git Change to the project directory: $ cd ML-Deploy Create a virtual environement: $ python3 -m venv env Install the dependencies: $ pip install -r requirements.txt Usage From source Run ML-Deploy using the command below: $ uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload Tests Run the test suite using the command below: console $ pytest --cov=app --cov-report term-missing","title":"Install and run"},{"location":"install-run/#getting-started","text":"System Requirements: Python : 3.x","title":"Getting Started"},{"location":"install-run/#installation","text":"","title":"Installation"},{"location":"install-run/#usage","text":"","title":"Usage"},{"location":"install-run/#tests","text":"Run the test suite using the command below: console $ pytest --cov=app --cov-report term-missing","title":"Tests"},{"location":"llm/","text":"chat_response ( prompt , context = '' , chat_history = [], impact_area = 'Non sp\u00e9cifi\u00e9' ) Processes a user's prompt to generate the assistant's response using GPT-4o-mini, with context about the environmental incident. Parameters: Name Type Description Default prompt str The user's message to which the assistant should respond. required context str A JSON string containing context about the incident. '' chat_history list The existing chat history for this session. [] impact_area str The area impacted by the incident. 'Non sp\u00e9cifi\u00e9' Returns: Name Type Description str The assistant's response. Examples: >>> context = '{\"type_incident\": \"D\u00e9forestation\", \"analysis\": \"La d\u00e9forestation affecte la biodiversit\u00e9 locale.\", \"piste_solution\": \"Reforestation et \u00e9ducation communautaire.\"}' >>> prompt = \"Quels sont les impacts de la d\u00e9forestation dans cette zone ?\" >>> chat_response ( prompt , context ) 'La d\u00e9forestation affecte la biodiversit\u00e9 locale en r\u00e9duisant les habitats naturels des esp\u00e8ces. Pour rem\u00e9dier \u00e0 cela, la reforestation et l'\u00e9ducation communautaire sont des pistes de solution envisageables.' >>> context = '{\"type_incident\": \"Pollution de l' eau \", \" analysis \": \" Les rejets industriels ont contamin\u00e9 la rivi\u00e8re . \", \" piste_solution \": \" Installation de stations de traitement des eaux . \"}' >>> prompt = \"Comment pouvons-nous am\u00e9liorer la qualit\u00e9 de l'eau ?\" >>> chat_response ( prompt , context ) 'Les rejets industriels ont contamin\u00e9 la rivi\u00e8re. Pour am\u00e9liorer la qualit\u00e9 de l'eau, l'installation de stations de traitement des eaux est recommand\u00e9e.' Source code in app/services/llm/gpt_3_5_turbo.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def chat_response ( prompt : str , context : str = \"\" , chat_history : list = [], impact_area : str = \"Non sp\u00e9cifi\u00e9\" ): \"\"\" Processes a user's prompt to generate the assistant's response using GPT-4o-mini, with context about the environmental incident. Args: prompt (str): The user's message to which the assistant should respond. context (str): A JSON string containing context about the incident. chat_history (list): The existing chat history for this session. impact_area (str): The area impacted by the incident. Returns: str: The assistant's response. Examples: >>> context = '{\"type_incident\": \"D\u00e9forestation\", \"analysis\": \"La d\u00e9forestation affecte la biodiversit\u00e9 locale.\", \"piste_solution\": \"Reforestation et \u00e9ducation communautaire.\"}' >>> prompt = \"Quels sont les impacts de la d\u00e9forestation dans cette zone ?\" >>> chat_response(prompt, context) 'La d\u00e9forestation affecte la biodiversit\u00e9 locale en r\u00e9duisant les habitats naturels des esp\u00e8ces. Pour rem\u00e9dier \u00e0 cela, la reforestation et l'\u00e9ducation communautaire sont des pistes de solution envisageables.' >>> context = '{\"type_incident\": \"Pollution de l'eau\", \"analysis\": \"Les rejets industriels ont contamin\u00e9 la rivi\u00e8re.\", \"piste_solution\": \"Installation de stations de traitement des eaux.\"}' >>> prompt = \"Comment pouvons-nous am\u00e9liorer la qualit\u00e9 de l'eau ?\" >>> chat_response(prompt, context) 'Les rejets industriels ont contamin\u00e9 la rivi\u00e8re. Pour am\u00e9liorer la qualit\u00e9 de l'eau, l'installation de stations de traitement des eaux est recommand\u00e9e.' \"\"\" # Parse the context JSON string to extract details about the incident context_obj = json . loads ( context ) incident_type = context_obj . get ( 'type_incident' , 'Inconnu' ) analysis = context_obj . get ( 'analysis' , 'Non sp\u00e9cifi\u00e9' ) piste_solution = context_obj . get ( 'piste_solution' , 'Non sp\u00e9cifi\u00e9' ) impact_summary = context_obj . get ( 'impact_summary' , 'Non sp\u00e9cifi\u00e9' ) # Update the system message to include the impact summary system_message = f \"\"\" <system> <role>assistant AI</role> <task>analyse des incidents environnementaux</task> <location>Mali</location> <incident> <type> { incident_type } </type> <analysis> { analysis } </analysis> <solution_tracks> { piste_solution } </solution_tracks> <impact_summary> { impact_summary } </impact_summary> </incident> <instructions> <instruction>Adaptez vos r\u00e9ponses au contexte sp\u00e9cifique de l'incident.</instruction> <instruction>Utilisez les informations de contexte pour enrichir vos explications.</instruction> <instruction>Int\u00e9grez les donn\u00e9es sur la zone d'impact dans vos analyses lorsque c'est pertinent.</instruction> <instruction>Si la question d\u00e9passe le contexte fourni, mentionnez clairement que vous r\u00e9pondez de mani\u00e8re g\u00e9n\u00e9rale.</instruction> <instruction>Priorisez les r\u00e9ponses concises et orient\u00e9es sur la r\u00e9solution du probl\u00e8me.</instruction> <instruction>Ne d\u00e9viez pas de la t\u00e2che principale et \u00e9vitez les r\u00e9ponses non pertinentes.</instruction> <response_formatting> <formatting_rule>R\u00e9pondez de mani\u00e8re concise, avec une longueur de r\u00e9ponse id\u00e9ale de 2 \u00e0 3 phrases.</formatting_rule> <formatting_rule>Fournissez une r\u00e9ponse structur\u00e9e : commencez par le probl\u00e8me principal, suivez avec la solution propos\u00e9e.</formatting_rule> <formatting_rule>Utilisez des mots simples et clairs, \u00e9vitez le jargon technique inutile.</formatting_rule> <formatting_rule>Donnez des informations essentielles en utilisant un langage direct et pr\u00e9cis.</formatting_rule> <formatting_rule>Si une recommandation est faite, assurez-vous qu'elle est faisable et contextualis\u00e9e.</formatting_rule> </response_formatting> </instructions> <examples> <example> <prompt>Quels sont les impacts de la d\u00e9forestation dans cette zone ?</prompt> <response>La d\u00e9forestation affecte la biodiversit\u00e9 locale en r\u00e9duisant les habitats naturels des esp\u00e8ces. Pour rem\u00e9dier \u00e0 cela, la reforestation et l'\u00e9ducation communautaire sont des pistes de solution envisageables.</response> </example> <example> <prompt>Comment pouvons-nous am\u00e9liorer la qualit\u00e9 de l'eau ?</prompt> <response>Les rejets industriels ont contamin\u00e9 la rivi\u00e8re. Pour am\u00e9liorer la qualit\u00e9 de l'eau, l'installation de stations de traitement des eaux est recommand\u00e9e.</response> </example> <example> <prompt>Que peut-on faire pour limiter l'\u00e9rosion des sols dans cette r\u00e9gion ?</prompt> <response>L'\u00e9rosion des sols est exacerb\u00e9e par la d\u00e9forestation et les pratiques agricoles non durables. Pour limiter l'\u00e9rosion, il est recommand\u00e9 de pratiquer l'agroforesterie, de planter des haies pour prot\u00e9ger les sols, et de promouvoir des techniques agricoles conservatrices.</response> </example> <example> <prompt>Quelles sont les cons\u00e9quences de la pollution de l'air sur la sant\u00e9 publique ici ?</prompt> <response>La pollution de l'air, principalement due aux \u00e9missions industrielles et \u00e0 la combustion de biomasse, a des effets n\u00e9gatifs sur la sant\u00e9 publique, notamment des probl\u00e8mes respiratoires et cardiovasculaires. Pour att\u00e9nuer ces impacts, il est essentiel de r\u00e9duire les sources d'\u00e9missions et de promouvoir des \u00e9nergies plus propres.</response> </example> <example> <prompt>Comment r\u00e9duire l'impact de la pollution plastique sur l'environnement local ?</prompt> <response>La pollution plastique peut \u00eatre r\u00e9duite en mettant en place des syst\u00e8mes de collecte et de recyclage des d\u00e9chets, en sensibilisant la population \u00e0 la r\u00e9duction de l'utilisation du plastique, et en favorisant des alternatives biod\u00e9gradables.</response> </example> <example> <prompt>Quelles mesures peuvent \u00eatre prises pour prot\u00e9ger la faune menac\u00e9e dans cette r\u00e9gion ?</prompt> <response>Pour prot\u00e9ger la faune menac\u00e9e, il est crucial de cr\u00e9er des zones prot\u00e9g\u00e9es, de lutter contre le braconnage, et de promouvoir des pratiques agricoles respectueuses de l'environnement qui minimisent la destruction des habitats naturels.</response> </example> <example> <prompt>Parlons de musique !</prompt> <response>Je comprends que vous souhaitez parler de musique. Toutefois, ma t\u00e2che principale est d'analyser les incidents environnementaux. Si vous avez des questions sur un incident environnemental, je serais ravi de vous aider.</response> </example> <example> <prompt>Avez-vous vu le dernier film populaire ?</prompt> <response>Je ne suis pas programm\u00e9 pour discuter des films r\u00e9cents. Mon r\u00f4le est de vous assister dans l'analyse des incidents environnementaux. Revenons \u00e0 ce sujet, si vous le voulez bien.</response> </example> <example> <prompt>Quels sont les meilleurs restaurants ici ?</prompt> <response>Je me concentre actuellement sur l'analyse des incidents environnementaux. Pour toute question relative aux incidents ou \u00e0 des pistes de solution, je suis pr\u00eat \u00e0 vous aider.</response> </example> <example> <prompt>Quelle est l'\u00e9tendue de la zone touch\u00e9e par cet incident ?</prompt> <response>L'analyse des donn\u00e9es satellitaires montre que la zone impact\u00e9e par cet incident couvre environ { impact_area } kilom\u00e8tres carr\u00e9s. Cette information nous aide \u00e0 mieux comprendre l'ampleur du probl\u00e8me et \u00e0 planifier des interventions appropri\u00e9es.</response> </example> </examples> </system> \"\"\" # Build the list of messages for the conversation with roles defined for each message messages = [ { \"role\" : \"system\" , \"content\" : system_message }, ] + chat_history + [{ \"role\" : \"user\" , \"content\" : prompt }] try : # Get the assistant's response response = client . chat . completions . create ( model = \"gpt-4o-mini\" , # Ensure the model is available and correctly specified messages = messages , temperature = 0.5 , # Reduce temperature for more focused and grounded responses max_tokens = 1080 , top_p = 0.8 , # Encourage more reliable answers by modifying top_p frequency_penalty = 0.3 , # Penalize repetition for more diverse outputs presence_penalty = 0.0 # Remove presence penalty to avoid deviation from the task ) assistant_response = response . choices [ 0 ] . message . content return assistant_response except Exception as e : print ( f \"An error occurred: { e } \" ) return \"D\u00e9sol\u00e9, je ne peux pas traiter votre demande pour le moment.\" display_chat_history ( messages ) Prints the chat history to the console. Each message is displayed with the sender's role and content. Parameters: Name Type Description Default messages list of dict A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. required Source code in app/services/llm/gpt_3_5_turbo.py 13 14 15 16 17 18 19 20 21 22 def display_chat_history ( messages ): \"\"\" Prints the chat history to the console. Each message is displayed with the sender's role and content. Args: messages (list of dict): A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. \"\"\" for message in messages : print ( f \" { message [ 'role' ] . capitalize () } : { message [ 'content' ] } \" ) generate_satellite_analysis ( ndvi_data , ndwi_data , landcover_data , incident_type ) Generate a detailed analysis of satellite data for environmental incidents using LLM, with proper markdown formatting. Parameters: Name Type Description Default ndvi_data DataFrame DataFrame containing NDVI data required ndwi_data DataFrame DataFrame containing NDWI data required landcover_data dict Dictionary containing land cover data required incident_type str Type of environmental incident required Returns: Name Type Description str Detailed analysis of the satellite data, formatted in markdown Source code in app/services/llm/gpt_3_5_turbo.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def generate_satellite_analysis ( ndvi_data , ndwi_data , landcover_data , incident_type ): \"\"\" Generate a detailed analysis of satellite data for environmental incidents using LLM, with proper markdown formatting. Args: ndvi_data (pd.DataFrame): DataFrame containing NDVI data ndwi_data (pd.DataFrame): DataFrame containing NDWI data landcover_data (dict): Dictionary containing land cover data incident_type (str): Type of environmental incident Returns: str: Detailed analysis of the satellite data, formatted in markdown \"\"\" # Prepare the context context = { \"type_incident\" : incident_type , \"ndvi_mean\" : ndvi_data [ 'NDVI' ] . mean (), \"ndvi_trend\" : 'augmentation' if ndvi_data [ 'NDVI' ] . iloc [ - 1 ] > ndvi_data [ 'NDVI' ] . iloc [ 0 ] else 'diminution' , \"ndwi_mean\" : ndwi_data [ 'NDWI' ] . mean (), \"ndwi_trend\" : 'augmentation' if ndwi_data [ 'NDWI' ] . iloc [ - 1 ] > ndwi_data [ 'NDWI' ] . iloc [ 0 ] else 'diminution' , \"dominant_cover\" : max ( landcover_data , key = landcover_data . get ), \"dominant_cover_percentage\" : landcover_data [ max ( landcover_data , key = landcover_data . get )] / sum ( landcover_data . values ()) * 100 } system_message = f \"\"\" <system> <role>assistant AI sp\u00e9cialis\u00e9 en analyse environnementale</role> <task>analyse des donn\u00e9es satellitaires pour incidents environnementaux avec formatage markdown</task> <incident> <type> { context [ 'type_incident' ] } </type> <ndvi_data> <mean> { context [ 'ndvi_mean' ] : .2f } </mean> <trend> { context [ 'ndvi_trend' ] } </trend> </ndvi_data> <ndwi_data> <mean> { context [ 'ndwi_mean' ] : .2f } </mean> <trend> { context [ 'ndwi_trend' ] } </trend> </ndwi_data> <landcover> <dominant> { context [ 'dominant_cover' ] } </dominant> <percentage> { context [ 'dominant_cover_percentage' ] : .1f } %</percentage> </landcover> </incident> <instructions> <instruction>Analysez les donn\u00e9es satellitaires fournies pour l'incident environnemental sp\u00e9cifi\u00e9.</instruction> <instruction>Interpr\u00e9tez les tendances NDVI et NDWI en relation avec le type d'incident.</instruction> <instruction>Expliquez l'importance de la couverture terrestre dominante dans le contexte de l'incident.</instruction> <instruction>Fournissez des insights sur les implications potentielles pour l'environnement local.</instruction> <instruction>Sugg\u00e9rez des pistes de solution ou des recommandations bas\u00e9es sur l'analyse.</instruction> <instruction>Formatez la r\u00e9ponse en utilisant la syntaxe markdown appropri\u00e9e.</instruction> </instructions> <response_formatting> <formatting_rule>Utilisez '**' pour les titres principaux. ex: **Titre**</formatting_rule> <formatting_rule>Utilisez '***texte***' pour mettre en gras et en italique les chiffres, pourcentages. ex: ***100***</formatting_rule> <formatting_rule>Utilisez '- ' au d\u00e9but d'une ligne pour les listes \u00e0 puces. ex: - item</formatting_rule> <formatting_rule>Laissez une ligne vide entre chaque paragraphe pour bien espacer le contenu.</formatting_rule> <formatting_rule>Structurez la r\u00e9ponse en sections claires avec des titres appropri\u00e9s.</formatting_rule> <formatting_rule>Utilisez des liens markdown si n\u00e9cessaire : [texte du lien](URL)</formatting_rule> </response_formatting> </system> \"\"\" user_prompt = f \"Analysez les donn\u00e9es satellitaires pour l'incident de type ' { incident_type } ' et fournissez un rapport d\u00e9taill\u00e9 format\u00e9 en markdown.\" messages = [ { \"role\" : \"system\" , \"content\" : system_message }, { \"role\" : \"user\" , \"content\" : user_prompt } ] try : response = client . chat . completions . create ( model = \"gpt-4o-mini\" , messages = messages , temperature = 0.7 , max_tokens = 2000 , top_p = 0.9 , frequency_penalty = 0.3 , presence_penalty = 0.0 ) analysis = response . choices [ 0 ] . message . content return analysis except Exception as e : print ( f \"An error occurred while generating satellite data analysis: { e } \" ) return \"D\u00e9sol\u00e9, une erreur s'est produite lors de l'analyse des donn\u00e9es satellitaires.\" get_assistant_response ( messages ) Sends the current chat history to the OpenAI API to generate a response from the assistant using GPT-4o-mini. Parameters: Name Type Description Default messages list of dict The current chat history as a list of message dictionaries. required Returns: Name Type Description str The assistant's response as a string. Raises: Type Description Exception Prints an error message if the API call fails and returns a default error response. Source code in app/services/llm/gpt_3_5_turbo.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_assistant_response ( messages ): \"\"\" Sends the current chat history to the OpenAI API to generate a response from the assistant using GPT-4o-mini. Args: messages (list of dict): The current chat history as a list of message dictionaries. Returns: str: The assistant's response as a string. Raises: Exception: Prints an error message if the API call fails and returns a default error response. \"\"\" try : r = client . chat . completions . create ( model = \"gpt-4o-mini\" , # The model version to use for generating responses messages = [{ \"role\" : m [ \"role\" ], \"content\" : m [ \"content\" ]} for m in messages ], temperature = 1 , # Adjust the temperature if needed max_tokens = 1080 , # Adjust as needed top_p = 1 , frequency_penalty = 0 , presence_penalty = 0 ) response = r . choices [ 0 ] . message . content return response except Exception as e : print ( f \"An error occurred: { e } \" ) return \"Sorry, I can't process your request right now.\" get_response ( prompt ) Processes a user's prompt to generate and display the assistant's response using GPT-4o-mini. Parameters: Name Type Description Default prompt str The user's message to which the assistant should respond. required Returns: Name Type Description str The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. Source code in app/services/llm/gpt_3_5_turbo.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_response ( prompt : str ): \"\"\" Processes a user's prompt to generate and display the assistant's response using GPT-4o-mini. Args: prompt (str): The user's message to which the assistant should respond. Returns: str: The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. \"\"\" # Add the user's message to the chat history messages . append ({ \"role\" : \"user\" , \"content\" : prompt }) # Get the assistant's response and add it to the chat history response = get_assistant_response ( messages ) messages . append ({ \"role\" : \"assistant\" , \"content\" : response }) # Display the updated chat history display_chat_history ( messages ) return response","title":"llm service"},{"location":"llm/#services.llm.gpt_3_5_turbo.chat_response","text":"Processes a user's prompt to generate the assistant's response using GPT-4o-mini, with context about the environmental incident. Parameters: Name Type Description Default prompt str The user's message to which the assistant should respond. required context str A JSON string containing context about the incident. '' chat_history list The existing chat history for this session. [] impact_area str The area impacted by the incident. 'Non sp\u00e9cifi\u00e9' Returns: Name Type Description str The assistant's response. Examples: >>> context = '{\"type_incident\": \"D\u00e9forestation\", \"analysis\": \"La d\u00e9forestation affecte la biodiversit\u00e9 locale.\", \"piste_solution\": \"Reforestation et \u00e9ducation communautaire.\"}' >>> prompt = \"Quels sont les impacts de la d\u00e9forestation dans cette zone ?\" >>> chat_response ( prompt , context ) 'La d\u00e9forestation affecte la biodiversit\u00e9 locale en r\u00e9duisant les habitats naturels des esp\u00e8ces. Pour rem\u00e9dier \u00e0 cela, la reforestation et l'\u00e9ducation communautaire sont des pistes de solution envisageables.' >>> context = '{\"type_incident\": \"Pollution de l' eau \", \" analysis \": \" Les rejets industriels ont contamin\u00e9 la rivi\u00e8re . \", \" piste_solution \": \" Installation de stations de traitement des eaux . \"}' >>> prompt = \"Comment pouvons-nous am\u00e9liorer la qualit\u00e9 de l'eau ?\" >>> chat_response ( prompt , context ) 'Les rejets industriels ont contamin\u00e9 la rivi\u00e8re. Pour am\u00e9liorer la qualit\u00e9 de l'eau, l'installation de stations de traitement des eaux est recommand\u00e9e.' Source code in app/services/llm/gpt_3_5_turbo.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def chat_response ( prompt : str , context : str = \"\" , chat_history : list = [], impact_area : str = \"Non sp\u00e9cifi\u00e9\" ): \"\"\" Processes a user's prompt to generate the assistant's response using GPT-4o-mini, with context about the environmental incident. Args: prompt (str): The user's message to which the assistant should respond. context (str): A JSON string containing context about the incident. chat_history (list): The existing chat history for this session. impact_area (str): The area impacted by the incident. Returns: str: The assistant's response. Examples: >>> context = '{\"type_incident\": \"D\u00e9forestation\", \"analysis\": \"La d\u00e9forestation affecte la biodiversit\u00e9 locale.\", \"piste_solution\": \"Reforestation et \u00e9ducation communautaire.\"}' >>> prompt = \"Quels sont les impacts de la d\u00e9forestation dans cette zone ?\" >>> chat_response(prompt, context) 'La d\u00e9forestation affecte la biodiversit\u00e9 locale en r\u00e9duisant les habitats naturels des esp\u00e8ces. Pour rem\u00e9dier \u00e0 cela, la reforestation et l'\u00e9ducation communautaire sont des pistes de solution envisageables.' >>> context = '{\"type_incident\": \"Pollution de l'eau\", \"analysis\": \"Les rejets industriels ont contamin\u00e9 la rivi\u00e8re.\", \"piste_solution\": \"Installation de stations de traitement des eaux.\"}' >>> prompt = \"Comment pouvons-nous am\u00e9liorer la qualit\u00e9 de l'eau ?\" >>> chat_response(prompt, context) 'Les rejets industriels ont contamin\u00e9 la rivi\u00e8re. Pour am\u00e9liorer la qualit\u00e9 de l'eau, l'installation de stations de traitement des eaux est recommand\u00e9e.' \"\"\" # Parse the context JSON string to extract details about the incident context_obj = json . loads ( context ) incident_type = context_obj . get ( 'type_incident' , 'Inconnu' ) analysis = context_obj . get ( 'analysis' , 'Non sp\u00e9cifi\u00e9' ) piste_solution = context_obj . get ( 'piste_solution' , 'Non sp\u00e9cifi\u00e9' ) impact_summary = context_obj . get ( 'impact_summary' , 'Non sp\u00e9cifi\u00e9' ) # Update the system message to include the impact summary system_message = f \"\"\" <system> <role>assistant AI</role> <task>analyse des incidents environnementaux</task> <location>Mali</location> <incident> <type> { incident_type } </type> <analysis> { analysis } </analysis> <solution_tracks> { piste_solution } </solution_tracks> <impact_summary> { impact_summary } </impact_summary> </incident> <instructions> <instruction>Adaptez vos r\u00e9ponses au contexte sp\u00e9cifique de l'incident.</instruction> <instruction>Utilisez les informations de contexte pour enrichir vos explications.</instruction> <instruction>Int\u00e9grez les donn\u00e9es sur la zone d'impact dans vos analyses lorsque c'est pertinent.</instruction> <instruction>Si la question d\u00e9passe le contexte fourni, mentionnez clairement que vous r\u00e9pondez de mani\u00e8re g\u00e9n\u00e9rale.</instruction> <instruction>Priorisez les r\u00e9ponses concises et orient\u00e9es sur la r\u00e9solution du probl\u00e8me.</instruction> <instruction>Ne d\u00e9viez pas de la t\u00e2che principale et \u00e9vitez les r\u00e9ponses non pertinentes.</instruction> <response_formatting> <formatting_rule>R\u00e9pondez de mani\u00e8re concise, avec une longueur de r\u00e9ponse id\u00e9ale de 2 \u00e0 3 phrases.</formatting_rule> <formatting_rule>Fournissez une r\u00e9ponse structur\u00e9e : commencez par le probl\u00e8me principal, suivez avec la solution propos\u00e9e.</formatting_rule> <formatting_rule>Utilisez des mots simples et clairs, \u00e9vitez le jargon technique inutile.</formatting_rule> <formatting_rule>Donnez des informations essentielles en utilisant un langage direct et pr\u00e9cis.</formatting_rule> <formatting_rule>Si une recommandation est faite, assurez-vous qu'elle est faisable et contextualis\u00e9e.</formatting_rule> </response_formatting> </instructions> <examples> <example> <prompt>Quels sont les impacts de la d\u00e9forestation dans cette zone ?</prompt> <response>La d\u00e9forestation affecte la biodiversit\u00e9 locale en r\u00e9duisant les habitats naturels des esp\u00e8ces. Pour rem\u00e9dier \u00e0 cela, la reforestation et l'\u00e9ducation communautaire sont des pistes de solution envisageables.</response> </example> <example> <prompt>Comment pouvons-nous am\u00e9liorer la qualit\u00e9 de l'eau ?</prompt> <response>Les rejets industriels ont contamin\u00e9 la rivi\u00e8re. Pour am\u00e9liorer la qualit\u00e9 de l'eau, l'installation de stations de traitement des eaux est recommand\u00e9e.</response> </example> <example> <prompt>Que peut-on faire pour limiter l'\u00e9rosion des sols dans cette r\u00e9gion ?</prompt> <response>L'\u00e9rosion des sols est exacerb\u00e9e par la d\u00e9forestation et les pratiques agricoles non durables. Pour limiter l'\u00e9rosion, il est recommand\u00e9 de pratiquer l'agroforesterie, de planter des haies pour prot\u00e9ger les sols, et de promouvoir des techniques agricoles conservatrices.</response> </example> <example> <prompt>Quelles sont les cons\u00e9quences de la pollution de l'air sur la sant\u00e9 publique ici ?</prompt> <response>La pollution de l'air, principalement due aux \u00e9missions industrielles et \u00e0 la combustion de biomasse, a des effets n\u00e9gatifs sur la sant\u00e9 publique, notamment des probl\u00e8mes respiratoires et cardiovasculaires. Pour att\u00e9nuer ces impacts, il est essentiel de r\u00e9duire les sources d'\u00e9missions et de promouvoir des \u00e9nergies plus propres.</response> </example> <example> <prompt>Comment r\u00e9duire l'impact de la pollution plastique sur l'environnement local ?</prompt> <response>La pollution plastique peut \u00eatre r\u00e9duite en mettant en place des syst\u00e8mes de collecte et de recyclage des d\u00e9chets, en sensibilisant la population \u00e0 la r\u00e9duction de l'utilisation du plastique, et en favorisant des alternatives biod\u00e9gradables.</response> </example> <example> <prompt>Quelles mesures peuvent \u00eatre prises pour prot\u00e9ger la faune menac\u00e9e dans cette r\u00e9gion ?</prompt> <response>Pour prot\u00e9ger la faune menac\u00e9e, il est crucial de cr\u00e9er des zones prot\u00e9g\u00e9es, de lutter contre le braconnage, et de promouvoir des pratiques agricoles respectueuses de l'environnement qui minimisent la destruction des habitats naturels.</response> </example> <example> <prompt>Parlons de musique !</prompt> <response>Je comprends que vous souhaitez parler de musique. Toutefois, ma t\u00e2che principale est d'analyser les incidents environnementaux. Si vous avez des questions sur un incident environnemental, je serais ravi de vous aider.</response> </example> <example> <prompt>Avez-vous vu le dernier film populaire ?</prompt> <response>Je ne suis pas programm\u00e9 pour discuter des films r\u00e9cents. Mon r\u00f4le est de vous assister dans l'analyse des incidents environnementaux. Revenons \u00e0 ce sujet, si vous le voulez bien.</response> </example> <example> <prompt>Quels sont les meilleurs restaurants ici ?</prompt> <response>Je me concentre actuellement sur l'analyse des incidents environnementaux. Pour toute question relative aux incidents ou \u00e0 des pistes de solution, je suis pr\u00eat \u00e0 vous aider.</response> </example> <example> <prompt>Quelle est l'\u00e9tendue de la zone touch\u00e9e par cet incident ?</prompt> <response>L'analyse des donn\u00e9es satellitaires montre que la zone impact\u00e9e par cet incident couvre environ { impact_area } kilom\u00e8tres carr\u00e9s. Cette information nous aide \u00e0 mieux comprendre l'ampleur du probl\u00e8me et \u00e0 planifier des interventions appropri\u00e9es.</response> </example> </examples> </system> \"\"\" # Build the list of messages for the conversation with roles defined for each message messages = [ { \"role\" : \"system\" , \"content\" : system_message }, ] + chat_history + [{ \"role\" : \"user\" , \"content\" : prompt }] try : # Get the assistant's response response = client . chat . completions . create ( model = \"gpt-4o-mini\" , # Ensure the model is available and correctly specified messages = messages , temperature = 0.5 , # Reduce temperature for more focused and grounded responses max_tokens = 1080 , top_p = 0.8 , # Encourage more reliable answers by modifying top_p frequency_penalty = 0.3 , # Penalize repetition for more diverse outputs presence_penalty = 0.0 # Remove presence penalty to avoid deviation from the task ) assistant_response = response . choices [ 0 ] . message . content return assistant_response except Exception as e : print ( f \"An error occurred: { e } \" ) return \"D\u00e9sol\u00e9, je ne peux pas traiter votre demande pour le moment.\"","title":"chat_response"},{"location":"llm/#services.llm.gpt_3_5_turbo.display_chat_history","text":"Prints the chat history to the console. Each message is displayed with the sender's role and content. Parameters: Name Type Description Default messages list of dict A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. required Source code in app/services/llm/gpt_3_5_turbo.py 13 14 15 16 17 18 19 20 21 22 def display_chat_history ( messages ): \"\"\" Prints the chat history to the console. Each message is displayed with the sender's role and content. Args: messages (list of dict): A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. \"\"\" for message in messages : print ( f \" { message [ 'role' ] . capitalize () } : { message [ 'content' ] } \" )","title":"display_chat_history"},{"location":"llm/#services.llm.gpt_3_5_turbo.generate_satellite_analysis","text":"Generate a detailed analysis of satellite data for environmental incidents using LLM, with proper markdown formatting. Parameters: Name Type Description Default ndvi_data DataFrame DataFrame containing NDVI data required ndwi_data DataFrame DataFrame containing NDWI data required landcover_data dict Dictionary containing land cover data required incident_type str Type of environmental incident required Returns: Name Type Description str Detailed analysis of the satellite data, formatted in markdown Source code in app/services/llm/gpt_3_5_turbo.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def generate_satellite_analysis ( ndvi_data , ndwi_data , landcover_data , incident_type ): \"\"\" Generate a detailed analysis of satellite data for environmental incidents using LLM, with proper markdown formatting. Args: ndvi_data (pd.DataFrame): DataFrame containing NDVI data ndwi_data (pd.DataFrame): DataFrame containing NDWI data landcover_data (dict): Dictionary containing land cover data incident_type (str): Type of environmental incident Returns: str: Detailed analysis of the satellite data, formatted in markdown \"\"\" # Prepare the context context = { \"type_incident\" : incident_type , \"ndvi_mean\" : ndvi_data [ 'NDVI' ] . mean (), \"ndvi_trend\" : 'augmentation' if ndvi_data [ 'NDVI' ] . iloc [ - 1 ] > ndvi_data [ 'NDVI' ] . iloc [ 0 ] else 'diminution' , \"ndwi_mean\" : ndwi_data [ 'NDWI' ] . mean (), \"ndwi_trend\" : 'augmentation' if ndwi_data [ 'NDWI' ] . iloc [ - 1 ] > ndwi_data [ 'NDWI' ] . iloc [ 0 ] else 'diminution' , \"dominant_cover\" : max ( landcover_data , key = landcover_data . get ), \"dominant_cover_percentage\" : landcover_data [ max ( landcover_data , key = landcover_data . get )] / sum ( landcover_data . values ()) * 100 } system_message = f \"\"\" <system> <role>assistant AI sp\u00e9cialis\u00e9 en analyse environnementale</role> <task>analyse des donn\u00e9es satellitaires pour incidents environnementaux avec formatage markdown</task> <incident> <type> { context [ 'type_incident' ] } </type> <ndvi_data> <mean> { context [ 'ndvi_mean' ] : .2f } </mean> <trend> { context [ 'ndvi_trend' ] } </trend> </ndvi_data> <ndwi_data> <mean> { context [ 'ndwi_mean' ] : .2f } </mean> <trend> { context [ 'ndwi_trend' ] } </trend> </ndwi_data> <landcover> <dominant> { context [ 'dominant_cover' ] } </dominant> <percentage> { context [ 'dominant_cover_percentage' ] : .1f } %</percentage> </landcover> </incident> <instructions> <instruction>Analysez les donn\u00e9es satellitaires fournies pour l'incident environnemental sp\u00e9cifi\u00e9.</instruction> <instruction>Interpr\u00e9tez les tendances NDVI et NDWI en relation avec le type d'incident.</instruction> <instruction>Expliquez l'importance de la couverture terrestre dominante dans le contexte de l'incident.</instruction> <instruction>Fournissez des insights sur les implications potentielles pour l'environnement local.</instruction> <instruction>Sugg\u00e9rez des pistes de solution ou des recommandations bas\u00e9es sur l'analyse.</instruction> <instruction>Formatez la r\u00e9ponse en utilisant la syntaxe markdown appropri\u00e9e.</instruction> </instructions> <response_formatting> <formatting_rule>Utilisez '**' pour les titres principaux. ex: **Titre**</formatting_rule> <formatting_rule>Utilisez '***texte***' pour mettre en gras et en italique les chiffres, pourcentages. ex: ***100***</formatting_rule> <formatting_rule>Utilisez '- ' au d\u00e9but d'une ligne pour les listes \u00e0 puces. ex: - item</formatting_rule> <formatting_rule>Laissez une ligne vide entre chaque paragraphe pour bien espacer le contenu.</formatting_rule> <formatting_rule>Structurez la r\u00e9ponse en sections claires avec des titres appropri\u00e9s.</formatting_rule> <formatting_rule>Utilisez des liens markdown si n\u00e9cessaire : [texte du lien](URL)</formatting_rule> </response_formatting> </system> \"\"\" user_prompt = f \"Analysez les donn\u00e9es satellitaires pour l'incident de type ' { incident_type } ' et fournissez un rapport d\u00e9taill\u00e9 format\u00e9 en markdown.\" messages = [ { \"role\" : \"system\" , \"content\" : system_message }, { \"role\" : \"user\" , \"content\" : user_prompt } ] try : response = client . chat . completions . create ( model = \"gpt-4o-mini\" , messages = messages , temperature = 0.7 , max_tokens = 2000 , top_p = 0.9 , frequency_penalty = 0.3 , presence_penalty = 0.0 ) analysis = response . choices [ 0 ] . message . content return analysis except Exception as e : print ( f \"An error occurred while generating satellite data analysis: { e } \" ) return \"D\u00e9sol\u00e9, une erreur s'est produite lors de l'analyse des donn\u00e9es satellitaires.\"","title":"generate_satellite_analysis"},{"location":"llm/#services.llm.gpt_3_5_turbo.get_assistant_response","text":"Sends the current chat history to the OpenAI API to generate a response from the assistant using GPT-4o-mini. Parameters: Name Type Description Default messages list of dict The current chat history as a list of message dictionaries. required Returns: Name Type Description str The assistant's response as a string. Raises: Type Description Exception Prints an error message if the API call fails and returns a default error response. Source code in app/services/llm/gpt_3_5_turbo.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_assistant_response ( messages ): \"\"\" Sends the current chat history to the OpenAI API to generate a response from the assistant using GPT-4o-mini. Args: messages (list of dict): The current chat history as a list of message dictionaries. Returns: str: The assistant's response as a string. Raises: Exception: Prints an error message if the API call fails and returns a default error response. \"\"\" try : r = client . chat . completions . create ( model = \"gpt-4o-mini\" , # The model version to use for generating responses messages = [{ \"role\" : m [ \"role\" ], \"content\" : m [ \"content\" ]} for m in messages ], temperature = 1 , # Adjust the temperature if needed max_tokens = 1080 , # Adjust as needed top_p = 1 , frequency_penalty = 0 , presence_penalty = 0 ) response = r . choices [ 0 ] . message . content return response except Exception as e : print ( f \"An error occurred: { e } \" ) return \"Sorry, I can't process your request right now.\"","title":"get_assistant_response"},{"location":"llm/#services.llm.gpt_3_5_turbo.get_response","text":"Processes a user's prompt to generate and display the assistant's response using GPT-4o-mini. Parameters: Name Type Description Default prompt str The user's message to which the assistant should respond. required Returns: Name Type Description str The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. Source code in app/services/llm/gpt_3_5_turbo.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_response ( prompt : str ): \"\"\" Processes a user's prompt to generate and display the assistant's response using GPT-4o-mini. Args: prompt (str): The user's message to which the assistant should respond. Returns: str: The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. \"\"\" # Add the user's message to the chat history messages . append ({ \"role\" : \"user\" , \"content\" : prompt }) # Get the assistant's response and add it to the chat history response = get_assistant_response ( messages ) messages . append ({ \"role\" : \"assistant\" , \"content\" : response }) # Display the updated chat history display_chat_history ( messages ) return response","title":"get_response"},{"location":"models/","text":"ImageModel Bases: BaseModel A model representing image data for processing and prediction in the API. Attributes: Name Type Description image_name str The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures List [ str ] A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id str A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. Source code in app/models/image_model.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class ImageModel ( BaseModel ): \"\"\" A model representing image data for processing and prediction in the API. Attributes: image_name (str): The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures (List[str]): A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id (str): A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. \"\"\" #Id: int # Uncomment if an ID attribute is necessary image_name : str sensitive_structures : List [ str ] incident_id : str zone : str latitude : float longitude : float","title":"Pydantic model"},{"location":"models/#models.image_model.ImageModel","text":"Bases: BaseModel A model representing image data for processing and prediction in the API. Attributes: Name Type Description image_name str The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures List [ str ] A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id str A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. Source code in app/models/image_model.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class ImageModel ( BaseModel ): \"\"\" A model representing image data for processing and prediction in the API. Attributes: image_name (str): The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures (List[str]): A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id (str): A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. \"\"\" #Id: int # Uncomment if an ID attribute is necessary image_name : str sensitive_structures : List [ str ] incident_id : str zone : str latitude : float longitude : float","title":"ImageModel"},{"location":"system-arch/","text":"","title":"System Arch"}]}