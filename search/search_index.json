{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hey! it's Map Action Map Action envisions a world where technology revolutionizes environmental management and rural & urban problem-solving, making sustainable living accessible to all communities in Mali and beyond. In a world increasingly challenged by environmental issues and urban complexities, Map Action envisions a future where cutting-edge technology and geospatial solutions empower communities, governments, and organizations. Our vision is to create a global society where sustainable urban and rural development and environmental stewardship are not only achievable but are the cornerstones of our collective well-being. Mission Statement Our mission is to develop, deploy, and promote open-source mapping tools and methodologies that enable individuals, communities, governments, and organizations to collaboratively identify, analyze, and solve critical environmental and urban challenges. Community Statement Map Action thrives on the strength of a diverse, inclusive community united by the goal of using technology for sustainable urban and environmental management. We commit to fostering an open, respectful environment where every voice is valued, and collaboration drives innovation. Together, we empower individuals and organizations to actively participate in crafting solutions that make a meaningful impact. Documentation Navigation API Routes Documentation Celery Configuration and Tasks Convolutional Neural Network (CNN) Components Contribution Guidelines Language Model (LLM) Component Image Model System Architecture Installation and Usage","title":"Welcom"},{"location":"#hey-its-map-action","text":"Map Action envisions a world where technology revolutionizes environmental management and rural & urban problem-solving, making sustainable living accessible to all communities in Mali and beyond. In a world increasingly challenged by environmental issues and urban complexities, Map Action envisions a future where cutting-edge technology and geospatial solutions empower communities, governments, and organizations. Our vision is to create a global society where sustainable urban and rural development and environmental stewardship are not only achievable but are the cornerstones of our collective well-being.","title":"Hey! it's Map Action"},{"location":"#documentation-navigation","text":"API Routes Documentation Celery Configuration and Tasks Convolutional Neural Network (CNN) Components Contribution Guidelines Language Model (LLM) Component Image Model System Architecture Installation and Usage","title":"Documentation Navigation"},{"location":"api_route/","text":"API Routes Documentation This document provides an overview of the API routes available in the Map Action project. These routes are defined in the main_router.py file and are responsible for handling various functionalities, including image predictions and WebSocket connections. Root Endpoint GET / Description : Verifies that the API is running. Response : Returns a message indicating the API is operational. Image Prediction POST /image/predict Description : Predicts the type of incident based on the provided image and other data. Request Body : Expects an ImageModel containing image name, sensitive structures, zone, and incident ID. Response : Returns prediction results, including incident type, probabilities, context, impact, and solution. WebSocket Chat WebSocket /ws/chat Description : Handles chat interactions and chat history deletion. Request : Expects a WebSocket connection with actions such as \"delete_chat\". Response : Sends chat responses and manages chat history. Chat History GET /MapApi/history/{chat_key} Description : Retrieves the chat history for a given chat key. Response : Returns a list of chat messages in chronological order. Additional Information Database Operations : The API interacts with a database to store and retrieve predictions and chat history. Error Handling : The API includes error handling for various operations, ensuring robust and reliable functionality. For more detailed information on each route, refer to the main_router.py file in the app/apis/ directory.","title":"API Routes Documentation"},{"location":"api_route/#api-routes-documentation","text":"This document provides an overview of the API routes available in the Map Action project. These routes are defined in the main_router.py file and are responsible for handling various functionalities, including image predictions and WebSocket connections.","title":"API Routes Documentation"},{"location":"api_route/#root-endpoint","text":"GET / Description : Verifies that the API is running. Response : Returns a message indicating the API is operational.","title":"Root Endpoint"},{"location":"api_route/#image-prediction","text":"POST /image/predict Description : Predicts the type of incident based on the provided image and other data. Request Body : Expects an ImageModel containing image name, sensitive structures, zone, and incident ID. Response : Returns prediction results, including incident type, probabilities, context, impact, and solution.","title":"Image Prediction"},{"location":"api_route/#websocket-chat","text":"WebSocket /ws/chat Description : Handles chat interactions and chat history deletion. Request : Expects a WebSocket connection with actions such as \"delete_chat\". Response : Sends chat responses and manages chat history.","title":"WebSocket Chat"},{"location":"api_route/#chat-history","text":"GET /MapApi/history/{chat_key} Description : Retrieves the chat history for a given chat key. Response : Returns a list of chat messages in chronological order.","title":"Chat History"},{"location":"api_route/#additional-information","text":"Database Operations : The API interacts with a database to store and retrieve predictions and chat history. Error Handling : The API includes error handling for various operations, ensuring robust and reliable functionality. For more detailed information on each route, refer to the main_router.py file in the app/apis/ directory.","title":"Additional Information"},{"location":"celery/","text":"Celery Configuration and Tasks This document provides an overview of the Celery configuration and tasks used in the Map Action project. Celery is used for distributed task processing, allowing for efficient handling of asynchronous operations. Celery Configuration The Celery application is configured in the celery_config.py file. It uses Redis as both the message broker and backend for task queueing and result storage. The configuration retrieves Redis connection details from environment variables. Key Components Redis : Used as the message broker and backend for Celery. Environment Variables : REDIS_HOST , REDIS_PORT , REDIS_USERNAME , REDIS_PASSWORD , and REDIS_URL are used to configure the Redis connection. Celery Tasks The Celery tasks are defined in the celery_task.py file. These tasks perform various operations, including image prediction, fetching contextual information, and analyzing incident zones. Tasks Overview perform_prediction : Uses a convolutional neural network to predict the content of an image and calculate classification probabilities. fetch_contextual_information : Fetches contextual information based on the prediction, sensitive structures, and geographic zone. analyze_incident_zone : Analyzes the incident zone using satellite data, generating analysis results and plot data. Task Details perform_prediction : Args : image (bytes) - The image data to be processed. Returns : A tuple containing the predicted classification and probabilities. fetch_contextual_information : Args : prediction (str), sensitive_structures (list), zone (str). Returns : A tuple containing analysis and piste_solution, both formatted in markdown. analyze_incident_zone : Args : lat (float), lon (float), incident_location (str), incident_type (str), start_date (str), end_date (str). Returns : A dictionary containing analysis results and plot data. For more detailed information on each task, refer to the celery_task.py file in the app/services/celery/ directory.","title":"celery config and tasks"},{"location":"celery/#celery-configuration-and-tasks","text":"This document provides an overview of the Celery configuration and tasks used in the Map Action project. Celery is used for distributed task processing, allowing for efficient handling of asynchronous operations.","title":"Celery Configuration and Tasks"},{"location":"celery/#celery-configuration","text":"The Celery application is configured in the celery_config.py file. It uses Redis as both the message broker and backend for task queueing and result storage. The configuration retrieves Redis connection details from environment variables.","title":"Celery Configuration"},{"location":"celery/#key-components","text":"Redis : Used as the message broker and backend for Celery. Environment Variables : REDIS_HOST , REDIS_PORT , REDIS_USERNAME , REDIS_PASSWORD , and REDIS_URL are used to configure the Redis connection.","title":"Key Components"},{"location":"celery/#celery-tasks","text":"The Celery tasks are defined in the celery_task.py file. These tasks perform various operations, including image prediction, fetching contextual information, and analyzing incident zones.","title":"Celery Tasks"},{"location":"celery/#tasks-overview","text":"perform_prediction : Uses a convolutional neural network to predict the content of an image and calculate classification probabilities. fetch_contextual_information : Fetches contextual information based on the prediction, sensitive structures, and geographic zone. analyze_incident_zone : Analyzes the incident zone using satellite data, generating analysis results and plot data.","title":"Tasks Overview"},{"location":"celery/#task-details","text":"perform_prediction : Args : image (bytes) - The image data to be processed. Returns : A tuple containing the predicted classification and probabilities. fetch_contextual_information : Args : prediction (str), sensitive_structures (list), zone (str). Returns : A tuple containing analysis and piste_solution, both formatted in markdown. analyze_incident_zone : Args : lat (float), lon (float), incident_location (str), incident_type (str), start_date (str), end_date (str). Returns : A dictionary containing analysis results and plot data. For more detailed information on each task, refer to the celery_task.py file in the app/services/celery/ directory.","title":"Task Details"},{"location":"cnn/","text":"Convolutional Neural Network (CNN) Components This document provides an overview of the CNN components used in the Map Action project. These components are responsible for preprocessing images, configuring the CNN model, and performing image classification. Preprocessing The preprocessing of images is handled by the cnn_preprocess.py file. It defines a function preprocess_image that resizes images to 224x224 pixels, converts them to PyTorch tensors, and optionally normalizes them. This preprocessing is tailored for models expecting input images in this format. Model Configuration The CNN model is configured in the cnn_model.py file. It defines a function m_a_model that initializes and modifies a ResNet50 model for multi-label classification. The model's fully connected layer is adjusted to match the number of tags to predict. Image Classification The image classification process is managed by the cnn.py file. It integrates the preprocessing and model components to perform multi-label image classification using the ResNet50 model. The file includes functions to load the model and perform predictions on images, returning the top predictions and their probabilities. Key Functions preprocess_image : Args : image (bytes) - The image data to be processed. Returns : A PyTorch tensor representing the processed image. m_a_model : Args : num_tags (int) - The number of tags to predict. Returns : A PyTorch model configured for multi-label classification. predict : Args : image (bytes) - The image data to be classified. Returns : A tuple containing a list of predicted tags and a list of probabilities. For more detailed information on each component, refer to the respective files in the app/services/cnn/ directory.","title":"cnn service"},{"location":"cnn/#convolutional-neural-network-cnn-components","text":"This document provides an overview of the CNN components used in the Map Action project. These components are responsible for preprocessing images, configuring the CNN model, and performing image classification.","title":"Convolutional Neural Network (CNN) Components"},{"location":"cnn/#preprocessing","text":"The preprocessing of images is handled by the cnn_preprocess.py file. It defines a function preprocess_image that resizes images to 224x224 pixels, converts them to PyTorch tensors, and optionally normalizes them. This preprocessing is tailored for models expecting input images in this format.","title":"Preprocessing"},{"location":"cnn/#model-configuration","text":"The CNN model is configured in the cnn_model.py file. It defines a function m_a_model that initializes and modifies a ResNet50 model for multi-label classification. The model's fully connected layer is adjusted to match the number of tags to predict.","title":"Model Configuration"},{"location":"cnn/#image-classification","text":"The image classification process is managed by the cnn.py file. It integrates the preprocessing and model components to perform multi-label image classification using the ResNet50 model. The file includes functions to load the model and perform predictions on images, returning the top predictions and their probabilities.","title":"Image Classification"},{"location":"cnn/#key-functions","text":"preprocess_image : Args : image (bytes) - The image data to be processed. Returns : A PyTorch tensor representing the processed image. m_a_model : Args : num_tags (int) - The number of tags to predict. Returns : A PyTorch model configured for multi-label classification. predict : Args : image (bytes) - The image data to be classified. Returns : A tuple containing a list of predicted tags and a list of probabilities. For more detailed information on each component, refer to the respective files in the app/services/cnn/ directory.","title":"Key Functions"},{"location":"contribution-guideline/","text":"Contributing to Model_deploy Thank you for considering contributing to Model_deploy! We welcome contributions from the community to help improve and enhance the project. Please take a moment to review this document to make the contribution process easy and effective for everyone involved. How to Contribute Reporting Issues If you encounter any bugs or have suggestions for improvements, please report them by creating an issue in the GitHub Issues section. When reporting an issue, please include: A clear and descriptive title. A detailed description of the problem or suggestion. Steps to reproduce the issue, if applicable. Any relevant logs or screenshots. Submitting Pull Requests We welcome pull requests for bug fixes, improvements, and new features. To submit a pull request: Fork the repository and clone your fork. Create a new branch for your changes: git checkout -b my-feature-branch Make your changes and commit them with clear and descriptive commit messages. Push your changes to your fork: git push origin my-feature-branch Open a pull request against the main branch of the original repository. Please ensure that your pull request: Follows the project's coding style and conventions. Includes tests for any new functionality or bug fixes. Does not introduce any new linting errors or warnings. Code of Conduct Please note that this project is governed by a Code of Conduct . By participating, you are expected to uphold this code. Please report any unacceptable behavior to the project maintainers. Additional Resources Developer Documentation Thank you for your contributions!","title":"contrubition guideline"},{"location":"contribution-guideline/#contributing-to-model_deploy","text":"Thank you for considering contributing to Model_deploy! We welcome contributions from the community to help improve and enhance the project. Please take a moment to review this document to make the contribution process easy and effective for everyone involved.","title":"Contributing to Model_deploy"},{"location":"contribution-guideline/#how-to-contribute","text":"","title":"How to Contribute"},{"location":"contribution-guideline/#reporting-issues","text":"If you encounter any bugs or have suggestions for improvements, please report them by creating an issue in the GitHub Issues section. When reporting an issue, please include: A clear and descriptive title. A detailed description of the problem or suggestion. Steps to reproduce the issue, if applicable. Any relevant logs or screenshots.","title":"Reporting Issues"},{"location":"contribution-guideline/#submitting-pull-requests","text":"We welcome pull requests for bug fixes, improvements, and new features. To submit a pull request: Fork the repository and clone your fork. Create a new branch for your changes: git checkout -b my-feature-branch Make your changes and commit them with clear and descriptive commit messages. Push your changes to your fork: git push origin my-feature-branch Open a pull request against the main branch of the original repository. Please ensure that your pull request: Follows the project's coding style and conventions. Includes tests for any new functionality or bug fixes. Does not introduce any new linting errors or warnings.","title":"Submitting Pull Requests"},{"location":"contribution-guideline/#code-of-conduct","text":"Please note that this project is governed by a Code of Conduct . By participating, you are expected to uphold this code. Please report any unacceptable behavior to the project maintainers.","title":"Code of Conduct"},{"location":"contribution-guideline/#additional-resources","text":"Developer Documentation Thank you for your contributions!","title":"Additional Resources"},{"location":"install-run/","text":"Getting Started System Requirements: Python : 3.x Installation From source Clone the ML-Deploy repository: $ git clone https://github.com/223MapAction/ML-Deploy.git Change to the project directory: $ cd ML-Deploy Create a virtual environement: $ python3 -m venv env Install the dependencies: $ pip install -r requirements.txt Usage From source Run ML-Deploy using the command below: $ uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload Tests Run the test suite using the command below: console $ pytest --cov=app --cov-report term-missing","title":"Install and run"},{"location":"install-run/#getting-started","text":"System Requirements: Python : 3.x","title":"Getting Started"},{"location":"install-run/#installation","text":"","title":"Installation"},{"location":"install-run/#usage","text":"","title":"Usage"},{"location":"install-run/#tests","text":"Run the test suite using the command below: console $ pytest --cov=app --cov-report term-missing","title":"Tests"},{"location":"llm/","text":"Language Model (LLM) Component This document provides an overview of the LLM component used in the Map Action project. The LLM is responsible for generating responses to user prompts and analyzing satellite data for environmental incidents. Key Functions get_response : Processes a user's prompt to generate and display the assistant's response using the GPT-4o-mini model. Args : prompt (str) - The user's message to which the assistant should respond. Returns : The assistant's response, which is also added to the chat history. chat_response : Generates the assistant's response using GPT-4o-mini, with context about the environmental incident. Args : prompt (str), context (str), chat_history (list), impact_area (str). Returns : The assistant's response. generate_satellite_analysis : Generates a detailed analysis of satellite data for environmental incidents using LLM, with proper markdown formatting. Args : ndvi_data (pd.DataFrame), ndwi_data (pd.DataFrame), landcover_data (dict), incident_type (str). Returns : Detailed analysis of the satellite data, formatted in markdown. Integration with OpenAI The LLM component uses the OpenAI API to generate responses. It initializes the OpenAI client with an API key from environment variables and interacts with the API to process chat history and generate responses. For more detailed information on each function, refer to the llm.py file in the app/services/llm/ directory.","title":"llm service"},{"location":"llm/#language-model-llm-component","text":"This document provides an overview of the LLM component used in the Map Action project. The LLM is responsible for generating responses to user prompts and analyzing satellite data for environmental incidents.","title":"Language Model (LLM) Component"},{"location":"llm/#key-functions","text":"get_response : Processes a user's prompt to generate and display the assistant's response using the GPT-4o-mini model. Args : prompt (str) - The user's message to which the assistant should respond. Returns : The assistant's response, which is also added to the chat history. chat_response : Generates the assistant's response using GPT-4o-mini, with context about the environmental incident. Args : prompt (str), context (str), chat_history (list), impact_area (str). Returns : The assistant's response. generate_satellite_analysis : Generates a detailed analysis of satellite data for environmental incidents using LLM, with proper markdown formatting. Args : ndvi_data (pd.DataFrame), ndwi_data (pd.DataFrame), landcover_data (dict), incident_type (str). Returns : Detailed analysis of the satellite data, formatted in markdown.","title":"Key Functions"},{"location":"llm/#integration-with-openai","text":"The LLM component uses the OpenAI API to generate responses. It initializes the OpenAI client with an API key from environment variables and interacts with the API to process chat history and generate responses. For more detailed information on each function, refer to the llm.py file in the app/services/llm/ directory.","title":"Integration with OpenAI"},{"location":"models/","text":"Image Model This document provides an overview of the ImageModel component used in the Map Action project. The ImageModel is a Pydantic model that represents image data for processing and prediction in the API. Attributes image_name (str): The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures (List[str]): A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id (str): A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. zone (str): The geographic zone related to the image and incident. latitude (float): The latitude coordinate of the incident location. longitude (float): The longitude coordinate of the incident location. The ImageModel is integral to the API's functionality, as it encapsulates all necessary data for image processing and prediction tasks. For more detailed information, refer to the image_model.py file in the app/models/ directory.","title":"Pydantic model"},{"location":"models/#image-model","text":"This document provides an overview of the ImageModel component used in the Map Action project. The ImageModel is a Pydantic model that represents image data for processing and prediction in the API.","title":"Image Model"},{"location":"models/#attributes","text":"image_name (str): The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures (List[str]): A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id (str): A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. zone (str): The geographic zone related to the image and incident. latitude (float): The latitude coordinate of the incident location. longitude (float): The longitude coordinate of the incident location. The ImageModel is integral to the API's functionality, as it encapsulates all necessary data for image processing and prediction tasks. For more detailed information, refer to the image_model.py file in the app/models/ directory.","title":"Attributes"},{"location":"system-arch/","text":"System Architecture This document provides an overview of the system architecture for the Map Action project. It outlines the key components and their interactions within the system. Overview The Map Action project is designed to leverage mapping technology and machine learning to address environmental challenges. The system is composed of several key components, each responsible for specific functionalities. Key Components API Layer : Built using FastAPI, this layer handles incoming requests and routes them to the appropriate services. Key files: main_router.py , image_model.py . Services : Celery : Manages asynchronous task processing for operations like image prediction and contextual information retrieval. Key files: celery_config.py , celery_task.py . CNN : Handles image preprocessing and classification using a convolutional neural network. Key files: cnn_preprocess.py , cnn_model.py , cnn.py . LLM : Utilizes a language model to generate responses and analyze satellite data. Key files: llm.py . Database : Stores prediction results, chat history, and other relevant data. Interacts with the API layer for data retrieval and storage. External Integrations : OpenAI API : Used by the LLM component for generating responses. Redis : Serves as the message broker and backend for Celery. Interaction Flow A request is received by the API layer, which validates and processes the input data. The request is routed to the appropriate service, such as the CNN for image classification or the LLM for generating responses. Asynchronous tasks are managed by Celery, allowing for efficient processing and response generation. Results are stored in the database and can be retrieved for further analysis or reporting. This architecture ensures a scalable and efficient system capable of handling complex environmental data and providing actionable insights. For more detailed information on each component, refer to the respective documentation files in the docs/ directory.","title":"System Arch"},{"location":"system-arch/#system-architecture","text":"This document provides an overview of the system architecture for the Map Action project. It outlines the key components and their interactions within the system.","title":"System Architecture"},{"location":"system-arch/#overview","text":"The Map Action project is designed to leverage mapping technology and machine learning to address environmental challenges. The system is composed of several key components, each responsible for specific functionalities.","title":"Overview"},{"location":"system-arch/#key-components","text":"API Layer : Built using FastAPI, this layer handles incoming requests and routes them to the appropriate services. Key files: main_router.py , image_model.py . Services : Celery : Manages asynchronous task processing for operations like image prediction and contextual information retrieval. Key files: celery_config.py , celery_task.py . CNN : Handles image preprocessing and classification using a convolutional neural network. Key files: cnn_preprocess.py , cnn_model.py , cnn.py . LLM : Utilizes a language model to generate responses and analyze satellite data. Key files: llm.py . Database : Stores prediction results, chat history, and other relevant data. Interacts with the API layer for data retrieval and storage. External Integrations : OpenAI API : Used by the LLM component for generating responses. Redis : Serves as the message broker and backend for Celery.","title":"Key Components"},{"location":"system-arch/#interaction-flow","text":"A request is received by the API layer, which validates and processes the input data. The request is routed to the appropriate service, such as the CNN for image classification or the LLM for generating responses. Asynchronous tasks are managed by Celery, allowing for efficient processing and response generation. Results are stored in the database and can be retrieved for further analysis or reporting. This architecture ensures a scalable and efficient system capable of handling complex environmental data and providing actionable insights. For more detailed information on each component, refer to the respective documentation files in the docs/ directory.","title":"Interaction Flow"}]}